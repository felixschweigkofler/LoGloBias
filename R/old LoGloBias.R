#' Create non-representative example data for a performance-focused task
#'
#' Create example trial-level local-global data from experiment in which a participant's performance is evaluated. The data does not represent any real patterns and merely severs to demonstrated the required structure.
#' @param mean.RT,sd.RT Overall mean and standard deviation values being passed to rnorm() to create example reaction times. Any negative values generated by rnorm() will be made positive. Default to 400 and 150, respectively.
#' @param mean.accuracy Overall mean accuracy used to create example accuracies. Defaults to 0.9.
#' @param level A vector of level identifiers, describing the level of EACH trial in an experiment. Is being used to determine (together with the length of 'congruence') the number of trials for each participant, for each session. Defaults to c(rep('loc',5), rep('glo',5)).
#' @param congruence A vector of level identifiers, describing which KINDS of congruence exists. Is being used to determine (together with the length of 'level') the number of trials. Defaults to c('con','inc').
#' @param id,session Vectors of participant and session identifiers. Default to 1:8 and 1:2 respectively.
#' @param format Defines if a true long format dataframe ('long') or a long-wide hybrid ('wide') is returned. Defaults to 'wide'.
#' @keywords example, data
#' @export
#' @examples
#' example.performance.data()
example.performance.data = function(mean.RT = 400, sd.RT = 150, mean.accuracy = 0.9, level = c(rep('loc',5), rep('glo',5)), congruence = c('con','inc'), id = 1:10, session = 1:2, format = 'wide', seed = NULL){
  set.seed(seed = seed)
  # Create a long dataframe of example trial data for 10 participants in a 2-session experiment with 5 local and 5 global trials, half of them congruent and half incongruent
  tr = expand.grid(list(measure = c('RT','ACC'), congruence = congruence, level = level, id = id, session = session))
  tr$trial = rep(1:(length(level)*length(congruence)), each = 2)
  tr$value = NA
  tr$value[seq(2,nrow(tr),2)] = sample(c(rep(0,round(1-mean.accuracy, 3) * 1000), rep(1,round(mean.accuracy, 3) * 1000)), nrow(tr)/2, replace = T)
  tr$value[seq(1,nrow(tr),2)] = abs(rnorm(nrow(tr)/2, mean = mean.RT, sd = sd.RT))
  tr$congruence = rep(as.vector(apply(matrix(tr[tr$measure == 'RT','congruence'], nrow = length(level)), 2, sample)), each = 2)
  tr = tr[,c('session','id','trial','level','congruence','measure','value')]
  if(format == 'wide') return(tidyr::pivot_wider(tr, names_from = 'measure')) else return(tr)
}


#' Create non-representative example data for a preference-focused task
#'
#' Create example trial-level local-global data from experiment in which a participant's preference is evaluated. The data does not represent any real patterns and merely severs to demonstrated the required structure.
#' @param mean.RT,sd.RT Overall mean and standard deviation values being passed to rnorm() to create example reaction times. Any negative values generated by rnorm() will be made positive. Default to 400 and 150, respectively.
#' @param mean.preference Overall mean preference used to create example preferences. Defaults to 0.5, which means equal chance of having a local or global preference.
#' @param congruence A vector of level identifiers, describing which KINDS of congruence exists. Is being used to determine (together with the length of 'level') the number of trials. Defaults to c('con','inc').
#' @param id,session Vectors of participant and session identifiers. Default to 1:8 and 1:2 respectively.
#' @param format Defines if a true long format dataframe ('long') or a long-wide hybrid ('wide') is returned. Defaults to 'wide'.
#' @keywords example, data
#' @export
#' @examples
#' example.preference.data()
example.preference.data = function(mean.RT = 400, sd.RT = 150, mean.preference = 0, congruence = c(rep('con',5), rep('inc',5)), id = 1:8, session = 1:2, format = 'wide', seed = NULL){
  set.seed(seed = seed)
  # Create a long dataframe of example trial data for 10 participants in a 2-session experiment with 5 local and 5 global trials, half of them congruent and half incongruent
  tr = expand.grid(list(measure = c('RT','PREF'), congruence = congruence, id = id, session = session))
  tr$trial = rep(1:length(congruence), each = 2)
  tr$value = NA
  tr$value[seq(2,nrow(tr),2)] = sample(c(rep(-1,1000-round(mean.preference*1000)), rep(1,1000-round(-mean.preference*1000))), nrow(tr)/2, replace = T)
  tr$value[seq(1,nrow(tr),2)] = abs(rnorm(nrow(tr)/2, mean = mean.RT, sd = sd.RT))
  tr$congruence = rep(as.vector(apply(matrix(tr[tr$measure == 'RT','congruence'], nrow = length(congruence)), 2, sample)), each = 2)
  tr = tr[,c('session','id','trial','congruence','measure','value')]
  if(format == 'wide') return(tidyr::pivot_wider(tr, names_from = 'measure')) else return(tr)
}


#' Return the default metric instructions
#'
#' A function that just returns hard-coded calculation instructions for the default metrics.
#' @keywords score
#' @export
#' @examples default.metrics()
default.metrics = function(){return(data.frame('OPS' = c('loc','glo','diff'),
                                               'OIS' = c('inc','con','diff'),
                                               'OYS' = c('inc','ctr','diff'),
                                               'OJS' = c('con','ctr','diff'),
                                               'LIS' = c('glo.inc','glo.con','diff'),
                                               'LYS' = c('glo.inc','glo.ctr','diff'),
                                               'LJS' = c('glo.con','glo.ctr','diff'),
                                               'GIS' = c('loc.inc','loc.con','diff'),
                                               'GYS' = c('loc.inc','loc.ctr','diff'),
                                               'GJS' = c('loc.con','loc.ctr','diff'),
                                               'IPS' = c('loc.inc','glo.inc','diff'),
                                               'CPS' = c('loc.con','glo.con','diff'),
                                               'NPS' = c('loc.ctr','glo.ctr','diff'),
                                               'BIS' = c('IPS','CPS','diff'),
                                               'BYS' = c('IPS','NPS','diff'),
                                               'BJS' = c('CPS','NPS','diff'),
                                               'SIS' = c('GIS','LIS','min'),
                                               'SYS' = c('GYS','LYS','min'),
                                               'SJS' = c('GJS','LJS','min'),
                                               row.names = c('R1','R2','method')))}


#' Return the default metric instructions
#'
#' A function that just returns hard-coded calculation instructions for the default metrics.
#' @keywords score
#' @export
#' @examples preference.metrics()
preference.metrics = function(){return(data.frame('OPS' = c('all',NA,NA),
                                                  'IPS' = c('inc',NA,NA),
                                                  'CPS' = c('con',NA,NA),
                                                  'NPS' = c('ctr',NA,NA),
                                                  'BIS' = c('IPS','CPS','diff'),
                                                  'BYS' = c('IPS','NPS','diff'),
                                                  'BJS' = c('CPS','NPS','diff'),
                                                  row.names = c('R1','R2','method')))}


#' Summarise trials of a local-global preference bias experiment
#'
#' Group trials of a local-global bias experiment in which a participant's preference instead of accuracy is measured. Group trials in a long format dataframe and apply defineable summary statistic functions to the measures. A trial's level is defined by the choice the participant made.
#' @param ldf Wide-long hybrid dataframe with the trial data. The columns defined by 'measure.cols' contain the values and the other columns are either irrelevant or are used as identifier-columns to index these values.
#' @param measure.cols Names of the columns containing the measures. Default to c('ACC','RT') for accuracy and reaction time. Values must be numeric. Accuracy must be coded as 1 or 0. The error rate is automatically calculated from accuracy.
#' @param accuracy,error_rate The names of the columns that contain accuracy and error rate. If accuracy exists, but not error rate, the error rate is automatically calculated and gets the named defined by 'error_rate'. Default to ACC and ER.
#' @param group.by,group.by0 Identifier-columns that are used create trial-groupings, within which statistics are calculated. The column names defined by group.by are used to create a power set (all possible combinations, including NULL), while the column names specified by group.by0 are added to each element of the power set. The resulting sets with combinations of column names are used iteratively when grouping the data (see examples). Default to NULL.
#' @param correct.trials.only Removes all values that are not accuracy or error rate from trials with an accuracy of 1. Defaults to TRUE.
#' @param calculate A 2*x dataframe with instructions how to calculate summary statistics within the groupings. Column names specify the name for the summary statistic, the first row specifies the measure to be summarised, and the second row the function to be used. The function must accept a vector and output a scalar. Defaults to data.frame(mean_ACC = c('ACC','mean'), ER = c('ER','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')).
#' @param make.trial_type Columns to be used to create a singular trial_type column. Can only specify 1 or 2 columns to be used (e.g. level and congruence). Defaults to NULL.
#' @importFrom dplyr '%>%'
#' @keywords summary
#' @export
#' @examples
#' # Group by the power set of session, id, level, and congruence. Use default 'calculate'-instructions.
#' ldf = summarise.trials(wdf = example.performance.data(), group.by = c('session','id','level','congruence'))
#' # Group by the power set of level and congruence and the normal set of session and id. Now, only summaries that differentiate by session and id are calculated (e.g. no overall summary over the entire data)
#' ldf = summarise.trials(wdf = example.performance.data(), group.by = c('level','congruence'), group.by0 = c('session','id'))
#' # Calculate the sd of RT for the entire dataset and no other summary. Also, use all RTs, not only the RTs from correct trials.
#' ldf = summarise.trials(wdf = example.performance.data(), calculate = data.frame(sd_of_all_RT = c('RT','sd')), correct.trials.only = F)
summarise.trials = function(wdf, group.by = NULL, group.by0 = NULL, fuse.by = NULL, correct.trials.only = T, calculate = data.frame(mean_ACC = c('ACC','mean'), mean_ER = c('ER','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')), measure.cols = c('ACC','RT'), accuracy = 'ACC', error_rate = 'ER', make.trial_type = NULL){

  for(i in measure.cols) if(!i %in% colnames(wdf)) stop("'measure.cols' requires [",i,"] but it does not exist in the column names of 'wdf'")
  if(!is.null(group.by)) if(any(!group.by %in% colnames(wdf))) stop("[",paste(group.by[!group.by %in% colnames(wdf)], collapse = ', '),"] is required by 'group.by' but not present in the colnames of 'wdf'")
  if(!is.null(group.by0)) if(any(!group.by0 %in% colnames(wdf))) stop("[",paste(group.by0[!group.by0 %in% colnames(wdf)], collapse = ', '),"] is required by 'group.by0' but not present in the colnames of 'wdf'")
  if(any(group.by %in% group.by0)) stop("'group.by' and 'group.by0' cannot contain the same colum names")

  rownames(calculate) = c('measure','method')
  if(any(!calculate['measure',] %in% c(measure.cols, error_rate))) stop("'calculate' requires ",paste(calculate['measure',!calculate['measure',] %in% c(measure.cols,error_rate)], collapse = ',')," which does not exist in the colnames of 'wdf'")

  if(any(!unlist(lapply(wdf[,measure.cols],is.numeric)))) stop("The measure columns must be numeric")

  if(accuracy %in% colnames(wdf)) if(any(!wdf[[accuracy]] %in% c(1,0,NA))) stop("The accuracy can only be coded as 0 (incorrect) or 1 (correct). Missing values must be NA.")
  if(!is.null(correct.trials.only)) if(!accuracy %in% colnames(wdf)) stop("To remove incorrect trials, 'accuracy' must index the column with the accuracy-data.")

  for(i in c('trial_type','sum.of.ACC','sum.of.RT')) if(i %in% colnames(wdf)) stop("'wdf' cannot contain a column called [",i,"], as it is used internally.")
  if(any(!make.trial_type %in% colnames(wdf))) stop("'make.trial_type' can only define column names that exist in wdf")
  if(length(make.trial_type) > 2) stop("'make.trial_type' can not define more than two columns")

  # If accuracy is present, calculate the error from the accuracy
  if(accuracy %in% colnames(wdf)) wdf[[error_rate]] = 1 - wdf[[accuracy]]

  # If required, trim the RT of wrong trials. Create wdf2 to use for the calculation of RCS
  wdf2 = wdf
  if(correct.trials.only) wdf[!wdf[[accuracy]] %in% 1, measure.cols[!measure.cols %in% c(accuracy,error_rate)]] = NA

  # Create the power set of group.by (NULL if group.by is NULL)
  a = list(NULL)
  if(!is.null(group.by))
    for(j in 1:length(group.by)) a = c(a, combn(group.by, m=j, simplify=F))
  # Add group.by0 to each element of the power set
  a = lapply(a, function(vec) c(vec,group.by0))
  message('Provided grouping instructions'); print(a)

  A = data.frame(matrix(nrow = 1, ncol = length(group.by)+length(group.by0)+2))
  colnames(A) = c(group.by0,group.by,'measure','value')
  # For each summary method and each combination of grouping columns
  for (i in 1:ncol(calculate))
    for (j in 1:length(a))
      suppressMessages({
        # Group the value-col by all cols defined in group.by and then call the method specified in calculate on the measure specified in calculate
        A0 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(a[[j]]))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T)))
        # Add a column with name of this measure and rowbind it to the main df
        A0$measure = colnames(calculate)[i]
        A = dplyr::bind_rows(A, A0)})
  # Remove the first empty row
  A = A[-1,]

  # Create the trial_type and reorder the colums if required
  if(!is.null(make.trial_type)) {
    if(length(make.trial_type) == 1) A$trial_type = A[[make.trial_type]] else A$trial_type = ifelse(is.na(A$level) & is.na(A$congruence), 'all', ifelse(is.na(A$level), as.character(A$congruence),ifelse(is.na(A$congruence), as.character(A$level),paste(A$level, A$congruence, sep = "."))))
    A = A[,c(group.by0,group.by,'trial_type','measure','value')]} else A = A[,c(group.by0,group.by,'measure','value')]

  return(as.data.frame(A))
}


#' Summarise trials of a local-global preference bias experiment
#'
#' Group trials of a local-global bias experiment in which a participant's preference instead of accuracy is measured. Group trials in a long format dataframe and apply defineable summary statistic functions to the measures. A trial's level is defined by the choice the participant made.
#' @param ldf Wide-long hybrid dataframe with the trial data. The columns defined by 'measure.cols' contain the values and the other columns are either irrelevant or are used as identifier-columns to index these values.
#' @param measure.cols Names of the columns containing the measures. Default to c('PREF','RT') for accuracy and reaction time. Values must be numeric. Accuracy must be coded as 1 or 0. The error rate is automatically calculated from accuracy.
#' @param level.col Name of the to-be-created column that will contain the chosen level (preference) in each trial. If the trials should be grouped by level (group.by/group.by0) for calculation of RTs, this name must be used for the level-column. Is ignored if it does not exist in group.by/group.by0. Defaults to 'level'.
#' @param preference.col Name of the column holding the preference-measure. This name must be used as measure-name in the 'calculate' instructions. Defaults to PREF.
#' @param group.by,group.by0 Identifier-columns that are used create trial-groupings, within which statistics are calculated. The column names defined by group.by are used to create a power set (all possible combinations, including NULL), while the column names specified by group.by0 are added to each element of the power set. The resulting sets with combinations of column names are used iteratively when grouping the data (see examples). Default to NULL.
#' @param calculate A 2*x dataframe with instructions how to calculate summary statistics within the groupings. Column names specify the name for the summary statistic, the first row specifies the measure to be summarised, and the second row the function to be used. The function must accept a vector and output a scalar. Defaults to data.frame(mean_ACC = c('ACC','mean'), ER = c('ER','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')).
#' @param make.trial_type Columns to be used to create a singular trial_type column. Can only specify 1 or 2 columns to be used (e.g. level and congruence). Defaults to NULL.
#' @importFrom dplyr '%>%'
#' @keywords summary
#' @export
#' @examples
#' # Group by the power set of session, id, level, and congruence. Use default 'calculate'-instructions.
#' ldf = summarise.trials.PREF(wdf = example.preference.data(), group.by = c('session','id','level','congruence'))
#' # Group by the power set of level and congruence and the normal set of session and id. Now, only summaries that differentiate by session and id are calculated (e.g. no overall summary over the entire data)
#' ldf = summarise.trials.PREF(wdf = example.preference.data(), group.by = c('level','congruence'), group.by0 = c('session','id'))
#' debug(summarise.trials.PREF)
#' summarise.trials.PREF(wdf = example.preference.data(seed = 2),group.by0 = c('session','id','level','congruence'), make.trial_type = c('level','congruence'))
summarise.trials.PREF = function(wdf, group.by = NULL, group.by0 = NULL, calculate = data.frame(mean_PREF = c('PREF','mean'), sum_PREF = c('PREF','sum'), mean_RT = c('RT','mean'), median_RT = c('RT','median')), measure.cols = c('PREF','RT'), level.col = 'level', preference.col = 'PREF', make.trial_type = NULL){

  #TODO: Offer alternative coding for PREF, namely as 0 1 instad of -1 1

  for(i in measure.cols) if(!i %in% colnames(wdf)) stop("'measure.cols' requires [",i,"] but it does not exist in the column names of 'wdf'")
  if(!is.null(group.by)) if(any(!group.by %in% c(colnames(wdf),level.col))) stop("[",paste(group.by[!group.by %in% colnames(wdf)], collapse = ', '),"] is required by 'group.by' but not present in the colnames of 'wdf'")
  if(!is.null(group.by0)) if(any(!group.by0 %in% c(colnames(wdf),level.col))) stop("[",paste(group.by0[!group.by0 %in% colnames(wdf)], collapse = ', '),"] is required by 'group.by0' but not present in the colnames of 'wdf'")
  if(any(group.by %in% group.by0)) stop("'group.by' and 'group.by0' cannot contain the same colum names")

  rownames(calculate) = c('measure','method')
  if(any(!calculate['measure',] %in% measure.cols)) stop("'calculate' requires ",paste(calculate['measure',!calculate['measure',] %in% measure.cols], collapse = ',')," which does not exist in the colnames of 'wdf'")
  if(!preference.col %in% measure.cols) stop("[",preference.col,"] is required by 'preference.col' but does not exist in 'measure.cols'")

  if(any(!unlist(lapply(wdf[,measure.cols],is.numeric)))) stop("The measure columns must be numeric")

  for(i in c('trial_type','sum.of.ACC','sum.of.RT')) if(i %in% colnames(wdf)) stop("'wdf' cannot contain a column called [",i,"], as it is used internally.")
  if(any(!make.trial_type %in% c(colnames(wdf),level.col))) stop("'make.trial_type' can only define column names that exist in 'wdf'")
  if(any(!make.trial_type %in% c(group.by,group.by0))) stop("'make.trial_type' can only define column names that exist in 'group.by' or 'group.by0'")
  if(length(make.trial_type) > 2) stop("'make.trial_type' can not define more than two columns")

  # Remove trials with NA in the preference column
  wdf = wdf[!is.na(wdf[[preference.col]]),]

  # Remove the level from group.by and group.by0
  group.by_PREF  = group.by[ !group.by  %in% level.col]
  group.by0_PREF = group.by0[!group.by0 %in% level.col]
  # Create the power set of group.by (NULL if group.by is NULL)
  a = list(NULL)
  if(!is.null(group.by_PREF))
    for(j in 1:length(group.by_PREF)) a = c(a, combn(group.by_PREF, m=j, simplify=F))
  # Add group.by0 to each element of the power set
  a = lapply(a, function(vec) c(vec,group.by0_PREF))
  message('Provided grouping instructions for PREF'); print(a)

  # Create the final dataframe A, with columns for all grouping variables plus measure and value cols
  A = data.frame(matrix(nrow = 1, ncol = length(group.by)+length(group.by0)+2))
  colnames(A) = c(group.by0,group.by,'measure','value')
  for (i in which(calculate['measure',] %in% preference.col))
    for(j in 1:length(a)) suppressMessages({
      # Group the value-col by all cols defined in group.by and then call the method specified in calculate on the measure specified in calculate
      A0 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(a[[j]]))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T)))
      # Add a column with name of this measure and rowbind it to the main df
      A0$measure = colnames(calculate)[i]
      A = dplyr::bind_rows(A, A0)})
  # Remove the first empty row
  A = A[-1,]

  # Recreate the standard level-column from the preference column
  if(level.col %in% c(group.by,group.by0)) {
    wdf[[level.col]] = ifelse(wdf[[preference.col]] == 1, 'glo', 'loc')
    A[[level.col]] = NA}

  # Create the power set of group.by (NULL if group.by is NULL)
  a = list(NULL)
  if(!is.null(group.by))
    for(j in 1:length(group.by)) a = c(a, combn(group.by, m=j, simplify=F))
  # Add group.by0 to each element of the power set
  a = lapply(a, function(vec) c(vec,group.by0))
  message('Provided grouping instructions for non-PREF'); print(a)

  # For each summary method and each combination of grouping columns
  for (i in which(!calculate['measure',] %in% preference.col))
    for (j in 1:length(a))
      suppressMessages({
        # Group the value-col by all cols defined in group.by and then call the method specified in calculate on the measure specified in calculate
        A0 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(a[[j]]))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T)), .groups = 'keep')
        # It can happen that a level does not exist for a participant or participant*congruence because the participant never preferred that level
        # To prevent empty rows, create a df containing all hypothetically possible combinations of groupings and left_join so that the missing values become NA
        A1 = expand.grid(lapply(A0[,a[[j]]],unique))
        A0 = dplyr::left_join(A1,A0)
        # Add a column with name of this measure and rowbind it to the main df
        A0$measure = colnames(calculate)[i]
        A = dplyr::bind_rows(A, A0)})

  # Create the trial_type and reorder the colums if required
  if(!is.null(make.trial_type)) {
    if(length(make.trial_type) == 1) A$trial_type = A[[make.trial_type]] else A$trial_type = ifelse(is.na(A$level) & is.na(A$congruence), 'all', ifelse(is.na(A$level), as.character(A$congruence),ifelse(is.na(A$congruence), as.character(A$level),paste(A$level, A$congruence, sep = "."))))
    A = A[,c(group.by0,group.by,'trial_type','measure','value')]} else A = A[,c(group.by0,group.by,'measure','value')]

  return(as.data.frame(A))
}




#' A function to summarise trials of a local-global bias experiment.
#'
#' A function to calculate bias metrics from summary values in a standardised long dataframe. The preference-measure is treated differently from all other measures. Interpret the meaning of metrics carefully, especially what a certain combination of metric and measure mean. Generally, when a large input measure signifies bad performance (e.g. long RT), a resulting positive score indicates a global bias (e.g. in CPS) or a bias for congruent figures (e.g. in LIS), respectively.
#' @param ldf Long format dataframe with one column named 'value' containing the values, one identifier-column named 'measure' describing the summary measure of the value (from summarise.trials()), and one identifer-column named 'type' which merges the information from level and congruence (from summarise.trials()).
#' @param identifier.cols Columns that uniquely index all values with their identifiers. No default.
#' @param metrics A 2*x dataframe with instructions how to calculate metrics as difference or minimum between two summary values. Each column contains instructions for one metric. The column name is the name of the metric to be calculated, the first and second row (R1, R2) specify either a summary type from the type-column in 'ldf' or a metric that was previously calculated, the third row specifies the method to be used to calculate the metric from the entries in row 1 and 2. Currently, only 'diff'  (score = R1 - R2) and 'min' (score = min(R1, R2, na.rm = F)) are implemented. When the function does not find the inputs required by a metric instruction (R1, R2), the metric is ignored. Defaults to default.metrics(), which returns a dataframe with instructios for default metrics, using the default summary types (con, inc, loc, glo, con.loc, con.glo, etc.).
#' @param custom.metrics A 2*x dataframe with an equivalent layout as default metrics, containing custom metric names and custom instructions. If a column name of 'custom.metrics' exists in 'default.metrics', it replaces the column, otherwise it is appended. A sorting algorithmus ensures that instructions are appended such that when a metric is used as minuend or subtrahend, this metric exists by the time it is required. Defaults to NULL.
#' @param preference.metrics A 2*x dataframe with an equivalent layout as default metrics, containing instructions for the calculation of preference-metrics. When the second and third row are NA, the metric as defined by the column name is identical to the value of the type/metric defined by the first row. Defaults to preference.metrics().
#' @param preference The identifiers in the measure-column used to index preference-summaries. No default.
#' @param notify Should progress information be provided. Defaults to TRUE.
#' @keywords summary
#' @export
#' @examples
#' summaries.perf = summarise.trials(     wdf = example.performance.data(), group.by = c('session','id','level','congruence'), make.trial_type = c('level','congruence'))
#' summaries.pref = summarise.trials.PREF(wdf = example.preference.data(),group.by0 = c('session','id','level','congruence'), make.trial_type = c('level','congruence'))
#' # Calculate trial type summaries from example.performance.data() with summarise.trials() and then calculate scores from these summaries
#' ldf = score.summaries(ldf = summaries.perf, identifier.cols = c('session','id','measure'))
#' # Calculate only OPS as difference between local and global trials
#' ldf = score.summaries(ldf = summaries.perf, identifier.cols = c('session','id','measure'), metrics = data.frame(OPS = c('loc','glo','diff')))
#' ldf = score.summaries(ldf = summaries, identifier.cols = c('session','id'), preference = c('mean_PREF','sum_PREF'))
#' debug(score.summaries)
score.summaries = function(ldf, identifier.cols, metrics = LoGloBias::default.metrics(), preference.metrics = LoGloBias::preference.metrics(), preference = NULL, measure.col = 'measure', trial_type.col = 'trial_type', notify = T){

  identifier.cols = unique(c(identifier.cols,measure.col))
  identifier.cols = identifier.cols[!identifier.cols %in% trial_type.col]
  if(measure.col != 'measure') stop("Currently, measure.col must be 'measure'")
  if(trial_type.col != 'trial_type') stop("Currently, trial_type.col must be 'trial_type'")
  if(any(!c('value','measure','trial_type') %in% colnames(ldf))) stop("'ldf' must contain cols named 'value', 'measure', and 'trial_type'")
  # TODO: Check if all groupings are equally-sized
  # TODO: Check if score.summaries requires equally-sized groupings
  # TODO: Implement free naming for measure col and trial_type col
  # TODO: Nicer notification of metrics, drop the splitting in diff and mins
  # TODO: Notification when a type or metric that is required does not exist (in the calculation loop); update info in the description

  ldf.type = ldf
  ind.type = lapply(ldf.type[,c(identifier.cols,'trial_type')], unique)

  # If present, separate the preference values from other values
  if(is.null(preference)) do.pref = F else if(any(!preference %in% ldf.type$measure)) stop("Not all names defined by 'preference' exist in the column [",measure.col,"]") else do.pref = T
  if(do.pref){
    if(notify) message("'",paste(preference,collapse = ','),"' found in column [measure]. Using separate calculation ('preference.metrics()') for the preference-based scores.")
    ldf.type.P = ldf.type[ ldf.type$measure %in% preference,]
    ldf.type  = ldf.type[!ldf.type$measure %in% preference,]
    ind.type.P = lapply(ldf.type.P[,c(identifier.cols,trial_type.col)], unique)
    ind.type  = lapply(ldf.type[ ,c(identifier.cols,trial_type.col)], unique)}

  # If the trial_types or metrics required for the calculation of a metric is neither present in ldf.type$trial_types nor in colnames(metrics), remove the metric from the list
  for (i in colnames(metrics)) if(any(!metrics[1:2,i] %in% ldf.type$trial_type)) if(any(!metrics[1:2,i] %in% colnames(metrics))) metrics[[i]] = NULL
  if(ncol(metrics)==0) {message("Metrics as instructed:"); print(metrics); stop("None of the trial trial_type names required were found in ldf. Please \n - either adhere to the naming convention 'loc' (local), 'glo' (global), 'con' (congruent), 'inc' (incongruent), 'ctr' (control), as well as 'loc.con' (local congruent), etc. \n - or redefine the names of the trial_types in by using 'custom.metrics'")}
  if(do.pref) {
    for (i in colnames(preference.metrics)) if(any(!preference.metrics[1:2,i] %in% c(ldf.type.P$trial_type,NA))) if(any(!preference.metrics[1:2,i] %in% colnames(preference.metrics))) preference.metrics[[i]] = NULL
    if(ncol(preference.metrics)==0) {message("Preference metrics as instructed:"); print(preference.metrics); stop("None of the trial trial_type names required were found in ldf. Please \n - either adhere to the naming convention 'loc' (local), 'glo' (global), 'con' (congruent), 'inc' (incongruent), 'ctr' (control), as well as 'loc.con' (local congruent), etc. \n - or redefine the names of the trial_types in by using 'custom.metrics'")}}

  # Notify of the calculations
  if(notify) {
    if(do.pref)                    {message("Preference calculation:");           print(preference.metrics)}
    if(any(metrics[3,] == 'diff')) {message("Metrics calculated as difference:"); print(metrics[,metrics[3,] == 'diff', drop = F], row.names = c('R1','R2','method'))}
    if(any(metrics[3,] == 'min'))  {message("Metrics calculated as min():");      print(metrics[,metrics[3,] == 'min',  drop = F], row.names = c('R1','R2','method'))}}

  ldf.metric = data.frame()
  # For each col in the standard metrics
  for (i in colnames(metrics)) {
    ab.val = list()
    # For first and second row of metrics[[i]], check if they are a trial trial_type or a metric, find and store the corresponding rows from ldf.type in a list
    for(j in 1:2) if(metrics[[i]][j] %in% unique(ldf.type$trial_type)) {ab.val[[j]] = ldf.type[ldf.type$trial_type %in% metrics[[i]][j],]} else if(metrics[[i]][j] %in% ldf.metric$metric) {ab.val[[j]] = ldf.metric[ldf.metric$metric %in% metrics[[i]][j],]}
    # Check the size of each part and left_join the two by group.by and measure
    if(nrow(ab.val[[1]]) != nrow(ab.val[[2]])) stop("Issue when calculating '",i,"': '",metrics[[i]][1],"' and '",metrics[[i]][2],"' have different rownumbers.")
    # ab.val = dplyr::left_join(ab.val[[1]],ab.val[[2]], by = identifier.cols)
    # if(metrics[[i]][3] == 'diff') ab.val$value = ab.val$value.x - ab.val$value.y else if(metrics[[i]][3] == 'ratio') ab.val$value = ab.val$value.x / ab.val$value.y else if(metrics[[i]][3] == 'ratio') ab.val$value = min(ab.val$value.x,ab.val$value.y) else stop("The method-row in the metric-instructions can only be 'diff', 'ratio', 'min', or NA")
    # for(j in 1:2) ab.val[[j]] = ab.val[[j]] %>% dplyr::arrange(!!dplyr::syms(identifier.cols))
    for (j in identifier.cols) if(any(ab.val[[1]][[j]] != ab.val[[2]][[j]], na.rm = T)) stop('Not properly sorted')
    # Calculate scores
    if(metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",metrics[[i]][3],"' is required to calculate '",colnames(metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
    val0 = ab.val[[1]][,identifier.cols]
    # Add metric name, value, and rbind
    val0$metric = i
    val0$value = val
    ldf.metric = rbind(ldf.metric, val0)}

  ldf.metric.P = data.frame()
  # For each column in the preference metrics
  if(do.pref) for(i in colnames(preference.metrics)){
    ab.val = list()
    # For first and second row of preference.metrics[[i]], check if they are a trial trial_type or a metric, find and store the corresponding rows from ldf.type in a list
    for(j in 1:2) if(is.na(preference.metrics[[i]][j])) ab.val[[j]] = NA else if(preference.metrics[[i]][j] %in% ldf.type.P$trial_type) ab.val[[j]] = ldf.type.P[ldf.type.P$trial_type %in% preference.metrics[[i]][j],] else if(preference.metrics[[i]][j] %in% ldf.metric.P$metric) ab.val[[j]] = ldf.metric.P[ldf.metric.P$metric %in% metrics[[i]][j],]
    # Check the size of each part and left_join the two by group.by and measure
    if(!is.na(ab.val[1]) & !is.na(ab.val[2])) if(nrow(ab.val[[1]]) != nrow(ab.val[[2]])) stop("Issue when calculating '",i,"': '",preference.metrics[[i]][1],"' and '",preference.metrics[[i]][2],"' have different rownumbers.")
    # ab.val = dplyr::left_join(ab.val[[1]],ab.val[[2]], by = identifier.cols)
    # if(preference.metrics[[i]][3] == 'diff') ab.val$value = ab.val$value.x - ab.val$value.y else if(preference.metrics[[i]][3] == 'ratio') ab.val$value = ab.val$value.x / ab.val$value.y else if(preference.metrics[[i]][3] == 'ratio') ab.val$value = min(ab.val$value.x,ab.val$value.y) else stop("The method-row in the metric-instructions can only be 'diff', 'ratio', 'min', or NA")
    # for(j in 1:2) ab.val[[j]] = ab.val[[j]] %>% dplyr::arrange(!!dplyr::syms(identifier.cols))
    if(!is.na(ab.val[1]) & !is.na(ab.val[2])) for (j in identifier.cols) if(any(ab.val[[1]][[j]] != ab.val[[2]][[j]], na.rm = T)) stop('Not properly sorted')
    # Calculate scores
    if(is.na(preference.metrics[[i]][3])) val = ab.val[[1]]$value else if(preference.metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(preference.metrics[[i]][3] == 'ratio') val = ab.val[[1]]$value / ab.val[[2]]$value else if(preference.metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",preference.metrics[[i]][3],"' is required to calculate '",colnames(preference.metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
    val0 = ab.val[[1]][,identifier.cols]
    # Add metric name, value, and rbind
    val0$metric = i
    val0$value = val
    ldf.metric.P = rbind(ldf.metric.P, val0)}

  # ab.val = list()
  # For R1 and R2, check if they are a trial trial_type or a metric, find and store the corresponding rows from ldf.type in a list
  # for(j in 1:2) if(is.na(preference.metrics[[i]][j])) ab.val[[j]] = NA else if(preference.metrics[[i]][j] %in% ind.type.P$trial_type) {ab.val[[j]] = ldf.type.P[ldf.type.P$trial_type %in% preference.metrics[[i]][j],]} else if(preference.metrics[[i]][j] %in% unique(ldf.metric.P$metric)) {ab.val[[j]] = ldf.metric.P[ldf.metric.P$metric %in% metrics[[i]][j],]}
  # Check if the order of the rows (except value, metric, trial_type) is the same in R1 and R2
  # if(!is.na(ab.val[1]) & !is.na(ab.val[2])) if(sum(!ab.val[[1]][,!colnames(ab.val[[1]]) %in% c('value', 'metric', 'trial_type')] == ab.val[[2]][,!colnames(ab.val[[2]]) %in% c('value', 'metric', 'trial_type')])>0) stop("The order of rows does not match in the calculation of ",i,". This error should not happen and possibly requires comprehensive troubleshooting of 'score.summaries()'")
  # For the id.dataframe extract the identifier-columns from the minuend ldfs, remove the trial_type-col (if it exists) and replace/add the target metric
  # for(j in 1:2) if(!is.na(ab.val[j])) id.df = ab.val[[j]][,!colnames(ab.val[[j]]) %in% c('value','trial_type')]
  # id.df$metric = i
  # If the metric i is not specified in metrics[[i]][3], calculate the difference between [[1]] and [[2]], otherwise take the min
  # if(is.na(preference.metrics[[i]][3])) val = ab.val[[1]]$value else if(preference.metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(preference.metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",preference.metrics[[i]][3],"' is required to calculate '",colnames(preference.metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
  # cbind the resulting var-vector and the id.dataframe and rbind the it with the ldf containing the previous metrics
  # ldf.metric.P = rbind(ldf.metric.P, cbind(id.df, value = val))

  # Join the two score dataframes
  ldf.metric = rbind(ldf.metric, ldf.metric.P)

  # Arrange the order of columns and then order the rows by column from left to right
  ldf.metric = ldf.metric[,c(identifier.cols,'metric','value')]
  for (i in rev(c(identifier.cols,'metric'))) ldf.metric = ldf.metric[order(ldf.metric[[i]]),]
  rownames(ldf.metric) = NULL

  return(ldf.metric)
}





#' A function to correlate observations
#'
#' A function to correlated observations in a long format dataframe, separately for chosen groupings. Returns the results of the correlation, as specified by the cor-column.
#' @param ldf Long format dataframe with one column named 'value' containing the values, one identifier-column specifying type of the value (such as a column defining the metric), and at least one identifier-column that can be used for grouping (such as participant id).
#' @param correlate.by Name of the column containing the identifiers that should be correlated with each other. Defaults to 'metric'.
#' @param group.by Name of the identifier-columns that are used to to group the observations. Each grouping is correlated internally by 'correlate.by'. E.g. group.by = c('sessions', 'measures') together with correlate.by = 'metric' means that metrics will be correlated with each other, for each session and each measure separately. Is mutually exclusive with collapse. Defaults to NULL.
#' @param collapse Name of the identifier-columns that are NOT used to to group the observations. E.g. collapse = c('sessions','participant id') together with correlate.by = 'metric' means that the metrics will be correlated for each session separately, but rather be pooled and 'session' vanishes as independent grouping in the results. Is mutually exclusive with group.by. Defaults to NULL.
#' @param method Method to be used by cor.test(). Choices are 'pearson', 'spearman', and 'kendall'. Defaults to 'pearson'.
#' @param notify Provides progress information. Defaults to TRUE.
#' @return s statistic, e.g. t-statistic for pearson
#' @return n number, degrees of freedom for pearson
#' @return p significance, p-value
#' @return c correlation, e.g. r for pearson
#' @return CI1,CI2 95% confidence interval
#' @keywords summary
#' @export
#' @examples
#' # Summarise trials to trial types, then calculate scores
#' scores = score.summaries(ldf = summarise.trials(ldf = example.performance.data()))
#' # Correlate metrics in the score-ldf by collapsing the id-column, ie. for all other combinations of columns (except metric and value) the correlations will be calculated separately
#' correlate.values(ldf = scores, correlate.by = 'metric', collapse = 'id')
#' # With the given data in scores, this is equivalent to grouping by session and measure
#' correlate.values(ldf = scores, correlate.by = 'metric', group.by = c('session', 'measure' ))
correlate.values = function(ldf, correlate.by = 'metric', group.by = NULL, collapse = NULL, method = 'pearson', notify = T){

  if(!'value' %in% colnames(ldf)) stop("'ldf' must be a long format dataframe with the column 'value' holding the values and all other columns being used to index the values")
  if(is.null(group.by) == is.null(collapse)) stop("EITHER 'group.by' OR 'collapse' can be used")
  if(any(!group.by %in% colnames(ldf))) stop(paste(group.by[!group.by %in% colnames(ldf)], collapse = ',')," is required by 'group.by' but does not exist in the colnames of 'ldf'")
  if(any(!correlate.by %in% colnames(ldf))) stop(paste(correlate.by[!correlate.by %in% colnames(ldf)], collapse = ',')," is required by 'correlate.by' but does not exist in the colnames of 'ldf'")
  if(any(!collapse %in% colnames(ldf))) stop(paste(collapse[!collapse %in% colnames(ldf)], collapse = ',')," is required by 'collapse' but does not exist in the colnames of 'ldf'")
  if(length(correlate.by)>1) stop("'correlate.by' can only specify one column")

  # If group.by was provided, the grouping cols group.by. else if collapse was provided, the grouping cols are all cols except correlate.by, collapse, and value, else the grouping cols are all cols except correlate.by, and value.
  if(!is.null(group.by)) group.by = group.by else if(!is.null(collapse)) group.by = colnames(ldf)[!colnames(ldf) %in% c(correlate.by, collapse, 'value')] else group.by = colnames(ldf)[!colnames(ldf) %in% c(correlate.by, 'value')]

  # Pivot wider by putting the values for each identifier in correlate.by into its own column
  x = ldf %>% tidyr::pivot_wider(names_from = dplyr::all_of(correlate.by), values_from = 'value')
  z = data.frame()
  # For each combination of the unique values in col 'correlate.by'
  fr = as.matrix(expand.grid(unique(ldf[,correlate.by]),unique(ldf[,correlate.by])))
  for (i in 1:nrow(fr)) {
    print(paste("Correlating",fr[i,1],"with",fr[i,2],"  ",round(i/nrow(fr),2)))
    suppressMessages({
      y = as.data.frame(x) %>%
        # Group the data as defined by group.by
        dplyr::group_by(dplyr::across(dplyr::all_of(group.by))) %>%
        # Calculate the correlation with cor.test and save the htest output as list element in the tibble
        dplyr::summarise(cortest=list(cor.test(.data[[fr[i,1]]], .data[[fr[i,2]]], method = method, na.rm = T))) %>%
        # Mutate each listelement by extracting the statistic s, the number of participants n, the p-value p, the correlation c, and the lower and upper confidence interval and storing each in its own colum
        mutate(
          corr = purrr::map(cortest, ~.[1:4]),
          s = purrr::map_dbl(cortest, ~.[[1]]),
          n = ifelse(method == 'pearson', purrr::map_dbl(cortest, ~.[[2]]), NA),
          p = purrr::map_dbl(cortest, ~.[[3]]),
          c = purrr::map_dbl(cortest, ~.[[4]]),
          CI1 = ifelse(method == 'pearson', purrr::map_dbl(cortest, ~.[[9]][1]), NA),
          CI2 = ifelse(method == 'pearson', purrr::map_dbl(cortest, ~.[[9]][2]), NA)) %>%
        # Remove unnecessary columns
        dplyr::select(-corr, -cortest)
    })
    # Add cols to store which elements of 'correlate.by' were correlated
    y[,paste0(correlate.by,'.1')] = fr[i,1]
    y[,paste0(correlate.by,'.2')] = fr[i,2]
    # Add to the full dataframe
    z = rbind(z,y)}
  # Turn the semi-ldf into a full ldf by putting all correlation statistics and values into a single value column and create a 'cor'-identifier col
  return(z %>% tidyr::pivot_longer(cols = c('s','n','p','c','CI1','CI2'), names_to = 'cor', values_to = 'value'))
}



#' A function to group observations and calculate statistics
#'
#' A function to group observations in a long format dataframe and calulate a range of statistics.
#' @param ldf Long format dataframe with one column named 'value' containing the values, one identifier-column specifying type of the value (such as a column defining the metric), and at least one identifier-column that can be used for grouping (such as participant id).
#' @param group.by Name of the identifier-columns that are used to to group the observations. Statistics are calculate for each grouping separately, e.g. group.by = c('sessions', 'measures') means that the statistic will be calculated separately for each session and each measure. Is mutually exclusive with collapse. Defaults to NULL.
#' @param collapse Name of the identifier-columns that are NOT used to to group the observations. E.g. collapse = c('sessions','participant id') together with correlate.by = 'metric' means that the metrics will be correlated for each session separately, but rather be pooled and 'session' vanishes as independent grouping in the results. Is mutually exclusive with group.by. Defaults to NULL.
#' @param statistics List of statistics to be applied to the groupings. The name of a list element is the statistic-name and the element defines the name of the function used to calculate it. Defaults to 'list(mean = 'mean', med = 'median', SD = 'sd', IQR = 'iqr', MAD = 'mad', skewness = 'skewness')'.
#' @keywords summary, statistic
#' @export
#' @examples
#' # Summarise trials to trial types, then calculate scores
#' scores = score.summaries(ldf = summarise.trials(ldf = example.performance.data()))
#' # Calculate statistics in the score-ldf by collapsing the id-column, i.e. for all other combinations of columns (except value) the statistic will be calculated separately
#' calculate.statistics(ldf = scores, collapse = 'id')
#' # With the given data in scores, this is equivalent to grouping by session and measure
#' calculate.statistics(ldf = scores, group.by = c('session', 'measure' ))
calculate.statistics = function(ldf, group.by = NULL, collapse = NULL, statistics = list(mean = 'mean', med = 'median', SD = 'sd', IQR = 'IQR', MAD = 'mad', skewness = 'skewness')){

  if(!'value' %in% colnames(ldf)) stop("'ldf' must be a long format dataframe with the column 'value' holding the values and all other columns being used to index the values")
  if(is.null(collapse) == is.null(group.by)) stop("EITHER 'collapse' OR 'group.by' can be used")
  if(any(!collapse %in% colnames(ldf))) stop(paste(collapse[!collapse %in% colnames(ldf)], collapse = ',')," is required by 'collapse' but does not exist in the colnames of 'ldf'")
  if(any(!group.by %in% colnames(ldf))) stop(paste(group.by[!group.by %in% colnames(ldf)], collapse = ',')," is required by 'group.by' but does not exist in the colnames of 'ldf'")

  # If collapse was provided, the grouping cols are all cols except collapse and value, else group.by are the grouping cols
  if(!is.null(collapse)) group.by = colnames(ldf)[!colnames(ldf) %in% c(collapse, 'value')] else if(!is.null(group.by)) group.by = group.by else group.by = colnames(ldf)[!colnames(ldf) %in% 'value']

  x = data.frame()
  for(i in 1:length(statistics)){
    # For each statistic, group the data by the group.by, add a col denoting the apply the statistic, and rbind with x
    y = ldf %>% dplyr::group_by(dplyr::across(dplyr::all_of(group.by))) %>% dplyr::summarise(value = do.call(statistics[[i]], list(value, na.rm = T)))
    y$statistic = names(statistics)[i]
    x = rbind(x, as.data.frame(y))}

  return(x)
}




# .----



#' Summarise trials of a local-global preference bias experiment
#'
#' Group trials of a local-global bias experiment in which a participant's preference instead of accuracy is measured. Group trials in a long format dataframe and apply defineable summary statistic functions to the measures. A trial's level is defined by the choice the participant made.
#' @param ldf Long format dataframe with the trial data. One column must contain the data (value.col), one column must specify which measure the value is (measure.col), one column must specify the trial number (trial.col), and two further columns must specify the level and congruence of the trial, respectively. Further columns are used to group the data. The presence of reaction time and accuracy measures is assumed, but not strictly required. All values must be numeric. Accuracy must be coded as 1 or 0. The error rate is automatically calculated from accuracy, if required by 'calculate'.
#' @param value.col Name of the column containing the data. Defaults to 'value'.
#' @param trial.col,congruence.col,level.col,measure.col Names of the columns that identify a data-point: the trial it comes from, the congruence and level of that trial, and which measure it is.
#' @param group.by Identifier-columns that are used create trial-groupings, within which statistics are calculated. All trials in a grouping must be uniquely identified. Defaults to NULL, which means all columns except the value.col, measure.col, congruence.col, and trial.col are used.
#' @param correct.trials.only Removes all values that are not accuracy from erroneous trials. Accuracy-calculation is not affected. Defaults to TRUE.
#' @param calculate A 2*x dataframe with instructions how to summarise trials. Column names specify the name of the summary statistic, the first row specifies the measure to be summarised, and the second row the function to be used. The function must accept a vector and output a scalar. Defaults to data.frame(mean_ACC = c('ACC','mean'), ER = c('ER','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')).
#' @param calculate.special A vector describing which special summary methods should be used. Special summary methods can not be calculated from a simple vector and therefore have to be written manually inside the function. Currently implemented are the inverse efficiecy score (mean_IES = mean_RT / mean_ACC) for the mean and median RT (mean_IES, median_IES) as well as the rate correct score (RCS = correct trials / second). RCS assumes reaction times are provided in milliseconds. Defaults to c('mean_IES','median_IES','RCS').
#' @keywords summary
#' @export
#' @examples
#' # Summarise the example trial data by sessiona and id. Use all default settings.
#' ldf = summarise.trials(ldf = example.performance.data(), group.by = c('session','id'))
#' # Since session and id are the only other columns besides the integral columns containing trial identifier, level and congruence identifier, the measure identifier, and the value, the result is the same without specifying the grouping
#' ldf = summarise.trials(ldf = example.performance.data())
#' # Summarise the example trial data, group by id (i.e. pool sessions)
#' ldf = summarise.trials(ldf = example.performance.data(), group.by = 'id')
#' # Note that if we do the same, but drop the session-column from the ldf, an error is thrown, because not all values are now uniquely identified, as each trial exists twice for each participant without a session-identifier to differentiate them
#' ldf = summarise.trials(ldf = example.performance.data()[,-1], group.by = 'id')
#' # Calculate only sd of RT.
#' ldf = summarise.trials(ldf = example.performance.data(), calculate = data.frame(SD = c('RT','sd')))
#' RCS is still automatically calculated. Disable by setting calculate.special to NULL.
#' ldf = summarise.trials(ldf = example.performance.data(), calculate = data.frame(SD = c('RT','sd')), calculate.special = NULL)
summarise.trials0 = function(ldf, group.by = NULL, correct.trials.only = T, calculate = data.frame(mean_ACC = c('ACC','mean'), mean_ER = c('ER','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')), calculate.special = c('mean_IES','median_IES','RCS'), trial.col = 'trial', measure.col = 'measure', congruence.col = 'congruence', level.col = 'level', value.col = 'value', reaction_time = 'RT', accuracy = 'ACC', error_rate = 'ER', notify = T){

  rownames(calculate) = c('measure','method')
  core.cols = list(trial.col = trial.col, measure.col = measure.col, congruence.col = congruence.col, level.col = level.col, value.col = value.col)
  for(i in names(core.cols)) if(!core.cols[[i]] %in% colnames(ldf)) stop("'",i,"' requires [",core.cols[[i]],"] but it does not exist in the column names of 'ldf'")
  if(!is.null(group.by)) if(any(!group.by %in% colnames(ldf))) stop("[",paste(group.by[!group.by %in% colnames(ldf)], collapse = ', '),"] is required by 'group.by' but not present in the colnames of 'ldf'")
  if(any(!calculate['measure',] %in% c(as.vector(ldf[[measure.col]]), error_rate))) stop("'calculate' requires ",paste(calculate['measure',!calculate['measure',] %in% ldf[[measure.col]]], collapse = ',')," which does not exist in [",measure.col,"]")
  if(any(!unique(ldf[ldf[[measure.col]] %in% accuracy,value.col,drop=T]) %in% c(0,1,NA))) stop("The preference can only be coded as 0 (local preference) or 1 (global preference). Missing values must be NA.")
  if(!is.null(calculate.special)) if(!any(calculate.special %in% c('mean_IES','median_IES','RCS'))) stop("Special summary methods are only implemented for 'mean_IES', 'median_IES', and 'RCS'. To implement your own special summary methods, please edit this function")
  if(!is.numeric(ldf[[value.col]])) stop("The value column must be numeric")
  if((correct.trials.only)){
    if(!accuracy %in% ldf[[measure.col]]) stop("To remove incorrect trials (does not affect accuracy scores), accuracy measures for each trial need to be provieded.")
    if(any(!dplyr::filter(ldf, measure == accuracy)[[value.col]] %in% c(0,1,NA))) stop("'",accuracy,"' may only be 0, 1, or NA")}
  if(error_rate %in% colnames(calculate) & !error_rate %in% ldf[[measure.col]] & !accuracy %in% ldf[[measure.col]]) stop("'",error_rate,"' is defined in 'calculate' but '",error_rate,"' does not exist in the [",measure.col,"] and neither does '",accuracy,"', from which '",error_rate,"' could be calculated.")
  for(i in c('trial_type','sum.of.ACC','sum.of.RT')) if(i %in% colnames(ldf)) stop("'ldf' cannot contain a column called [",i,"], as it is used internally.")

  # If no grouping was defined, use all cols except 'value','measure','level','congruence' and 'trial' for grouping
  if(is.null(group.by)) {
    group.by = colnames(ldf)[!colnames(ldf) %in% c(value.col,measure.col,level.col,congruence.col,trial.col)]
    if(notify) message("'group.by' is NULL. Using [",paste(group.by, collapse = ','),"] to group the trials.")}

  # Check trials are uniquely identified
  grpsz = ldf %>% dplyr::select(-!!dplyr::sym(value.col),-!!dplyr::sym(level.col),-!!dplyr::sym(congruence.col)) %>% dplyr::group_by_all() %>% dplyr::group_size()
  if(any(grpsz > 1)) stop(sum(grpsz - 1)," of ",sum(grpsz)," rows are duplicates of existing rows. Every row must be uniquely identified when using all columns except [",paste(level.col,congruence.col,value.col, sep = ','),"].")

  # Make the ldf wider and drop all rows that contain NA in the new accuracy-column
  wdf = ldf %>% tidyr::pivot_wider(names_from = all_of(measure.col), values_from = all_of(value.col))
  wdf[is.na(wdf[[accuracy]]),unique(ldf[[measure.col]])] = NA

  # If required, calculate the error from the accuracy
  if(error_rate %in% colnames(calculate) & !error_rate %in% ldf[[measure.col]]) wdf[[error_rate]] = 1 - wdf[[accuracy]]

  # If required, trim the RT of wrong trials. Create wdf2 to use for the calculation of RCS
  wdf2 = wdf
  if(correct.trials.only) wdf[!wdf[[accuracy]] %in% 1, unique(ldf[[measure.col]])[unique(ldf[[measure.col]]) != accuracy]] = NA

  A = data.frame()
  # For each col in the summary methods
  for (i in 1:ncol(calculate)) {suppressMessages({
    # Group the 'value' col by all cols defined in group.by / +level / +congruence / +level&congruence  and then call the method specified in calculate on the measure specified in calculate, and create a new column 'trial_type' that merges information from level and congruence
    A1 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(  group.by)))                           %>% dplyr::summarise(!!dplyr::sym(value.col) := do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); A1$trial_type = 'all'
    A2 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by,level.col))))                %>% dplyr::summarise(!!dplyr::sym(value.col) := do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); colnames(A2)[colnames(A2) == level.col] = 'trial_type'
    A3 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by,congruence.col))))           %>% dplyr::summarise(!!dplyr::sym(value.col) := do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); colnames(A3)[colnames(A3) == congruence.col] = 'trial_type'
    A4 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by,level.col,congruence.col)))) %>% dplyr::summarise(!!dplyr::sym(value.col) := do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); A4$trial_type = interaction(A4[[level.col]],A4[[congruence.col]]); A4[[congruence.col]] = NULL; A4[[level.col]] = NULL
    Ax = rbind(A1, A2, A3, A4)
    Ax[[measure.col]] = colnames(calculate)[i]
    A = rbind(A, Ax)})}


  # Calculate mean_IES and median_IES (inverse efficiency score = speed/accuracy)
  if(any(c('mean_IES','median_IES') %in% calculate.special)){
    skip.M = skip.m = F
    if(!'mean_ACC' %in% A[[measure.col]]) {skip.M = skip.m = T; message("Skipping calculation of '",paste(c('mean_IES','median_IES')[c('mean_IES','median_IES') %in% calculate.special],collapse = "','"),"' because 'mean_ACC', which is required, is not specified by 'calculate'")}
    if(!skip.M) if(!'mean_RT' %in% A[[measure.col]] & 'mean_IES' %in% calculate.special) {skip.M = T; message("Skipping calculation of 'mean_IES' because 'mean_RT', which is required, is not specified by 'calculate'")}
    if(!skip.m) if(!'median_RT' %in% A[[measure.col]] & 'median_IES' %in% calculate.special) {skip.m = T; message("Skipping calculation of 'median_IES' because 'median_RT', which is required, is not specified by 'calculate'")}
    if(!skip.m | !skip.M){
      # Get the current measures, needed for later tidyr::pivot_longer
      curr.meas = unique(A[[measure.col]])
      # Pivot wider by putting the values for each identifier in 'measure' into its own column
      A = as.data.frame(tidyr::pivot_wider(A, names_from = measure.col, values_from = value.col))
      # Add a new column by dividing the RT-col by the mean_ACC-col
      if(!skip.M) {A[,'mean_IES']   = A[,'mean_RT']   / A[,'mean_ACC']; curr.meas = c(curr.meas, 'mean_IES')}
      if(!skip.M) {A[,'median_IES'] = A[,'median_RT'] / A[,'mean_ACC']; curr.meas = c(curr.meas, 'median_IES')}
      # Make the wdf longer by collecting all the measure-and turning them into a full long df again, with the values stored in the value-col
      A = tidyr::pivot_longer(A, cols = dplyr::all_of(curr.meas), names_to = measure.col, values_to = value.col)}}


  # Calculate RCS (correct trials per second)
  if('RCS' %in% calculate.special){
    if(any(!c(reaction_time,accuracy) %in% ldf[[measure.col]])) stop("Skipping calculation of 'RCS', because ",paste(c(reaction_time,accuracy)[c(reaction_time,accuracy) %in% ldf[[measure.col]]],collapse = "','")," is missing in [",measure.col,"].")
    # Calculate the sum of reaction time and accuracy of each grouping from wdf2
    suppressMessages({
      RCS1 = wdf2 %>% dplyr::group_by(dplyr::across(dplyr::all_of(  group.by)))                             %>% dplyr::summarise(sum.of.RT = sum(!!dplyr::sym(reaction_time), na.rm = T), sum.of.ACC := sum(!!dplyr::sym(accuracy), na.rm = T)); RCS1$trial_type = 'all'
      RCS2 = wdf2 %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by, level.col))))                 %>% dplyr::summarise(sum.of.RT = sum(!!dplyr::sym(reaction_time), na.rm = T), sum.of.ACC := sum(!!dplyr::sym(accuracy), na.rm = T)); colnames(RCS2)[colnames(RCS2) == level.col] = 'trial_type'
      RCS3 = wdf2 %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by, congruence.col))))            %>% dplyr::summarise(sum.of.RT = sum(!!dplyr::sym(reaction_time), na.rm = T), sum.of.ACC := sum(!!dplyr::sym(accuracy), na.rm = T)); colnames(RCS3)[colnames(RCS3) == congruence.col] = 'trial_type'
      RCS4 = wdf2 %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by, level.col, congruence.col)))) %>% dplyr::summarise(sum.of.RT = sum(!!dplyr::sym(reaction_time), na.rm = T), sum.of.ACC := sum(!!dplyr::sym(accuracy), na.rm = T)); RCS4$trial_type = interaction(RCS4[[level.col]],RCS4[[congruence.col]]); RCS4[[congruence.col]] = NULL; RCS4[[level.col]] = NULL
      RCS = rbind(RCS1, RCS2, RCS3, RCS4)})
    # Scale the reaction time to seconds and calculate the RCS
    RCS[[value.col]] = RCS$sum.of.ACC / (RCS$sum.of.RT / 1000)
    # Add the measure.col, remove the help-columns and rbind with the other dataframe
    RCS[[measure.col]] = 'RCS'
    RCS$sum.of.ACC = NULL; RCS$sum.of.RT = NULL
    A = rbind(A, RCS)}

  # Arrange the order of columns and then order the rows numerically/alphabetically by column from left to right
  A = A[,c(group.by,'trial_type',measure.col,value.col)]
  for (i in rev(colnames(A)[!colnames(A) %in% value.col])) A = A[order(A[[i]]),]

  return(as.data.frame(A))
}

#' Summarise trials of a local-global preference bias experiment
#'
#' Group trials of a local-global bias experiment in which a participant's preference instead of accuracy is measured. Group trials in a long format dataframe and apply defineable summary statistic functions to the measures. A trial's level is defined by the choice the participant made.
#' @param ldf Long format dataframe with the trial data. One column must contain the data (value.col), one column must specify which measure the value is (measure.col), one column must specify the trial number (trial.col), and two further columns must specify the level and congruence of the trial, respectively. Further columns are used to group the data. The presence of reaction time and accuracy measures is assumed, but not strictly required. All values must be numeric. Accuracy must be coded as 1 or 0. The error rate is automatically calculated from accuracy, if required by 'calculate'.
#' @param value.col,measure.col Name of the columns containing the data and the measure-type (e.g. reaction time and accuracy) of the data. Default to 'value' and 'measure'.
#' @param group.by,group.by0 Identifier-columns that are used create trial-groupings, within which statistics are calculated. The column names defined by group.by are used to create a power set (all possible combinations, including NULL), while the column names specified by group.by0 are added to each element of the power set. The resulting sets with combinations of column names are used iteratively when grouping the data (see examples). Default to NULL.
#' @param correct.trials.only Removes all values that are not accuracy or error rate from trials with an accuracy of 1. Defaults to TRUE.
#' @param calculate A 2*x dataframe with instructions how to calculate summary statistics within the groupings. Column names specify the name for the summary statistic, the first row specifies the measure to be summarised, and the second row the function to be used. The function must accept a vector and output a scalar. Defaults to data.frame(mean_ACC = c('ACC','mean'), ER = c('ER','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')).
#' @param reaction_time,accuracy,error_rate The identifiers in the measure-column that mark rows containing a reaction_time-value, an accuracy-value, or an error rate value. Default to 'RT', 'ACC', and 'ER' Error rates are automatically calculated from accuracies.
#' @param make.trial_type Columns to be used to create a singular trial_type column. Can only specify 1 or 2 columns to be used (e.g. level and congruence). Defaults to NULL.
#' @keywords summary
#' @export
#' @examples
#' # Group by the power set of session, id, level, and congruence. Use default 'calculate'-instructions.
#' ldf = summarise.trials(ldf = example.performance.data(), group.by = c('session','id','level','congruence'))
#' # Group by the power set of level and congruence and the normal set of session and id. Now, only summaries that differentiate by session and id are calculated (e.g. no overall summary over the entire data)
#' ldf = summarise.trials(ldf = example.performance.data(), group.by = c('level','congruence'), group.by0 = c('session','id'))
#' # Group by id
#' ldf = summarise.trials(ldf = example.performance.data(), group.by = c('level','congruence'), group.by0 = 'id')
#' # Note that if we do the same, but drop the session-column from the ldf, an error is thrown, because rows are no longer identified
#' ldf = summarise.trials(ldf = example.performance.data()[,-1], group.by = c('level','congruence'), group.by0 = 'id')
#' # Calculate the sd of RT for the entire dataset and no other summary. Also, use all RTs, not only the RTs from correct trials.
#' ldf = summarise.trials(ldf = example.performance.data(), calculate = data.frame(sd_of_all_RT = c('RT','sd')), correct.trials.only = F)
summarise.trials_long.df = function(ldf, group.by = NULL, group.by0 = NULL, fuse.by = NULL, correct.trials.only = T, calculate = data.frame(mean_ACC = c('ACC','mean'), mean_ER = c('ER','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')), measure.col = 'measure', value.col = 'value', reaction_time = 'RT', accuracy = 'ACC', error_rate = 'ER', make.trial_type = NULL){

  rownames(calculate) = c('measure','method')
  core.cols = list(measure.col = measure.col, value.col = value.col)
  for(i in names(core.cols)) if(!core.cols[[i]] %in% colnames(ldf)) stop("'",i,"' requires [",core.cols[[i]],"] but it does not exist in the column names of 'ldf'")
  if(!is.null(group.by)) if(any(!group.by %in% colnames(ldf))) stop("[",paste(group.by[!group.by %in% colnames(ldf)], collapse = ', '),"] is required by 'group.by' but not present in the colnames of 'ldf'")
  if(!is.null(group.by0)) if(any(!group.by0 %in% colnames(ldf))) stop("[",paste(group.by0[!group.by0 %in% colnames(ldf)], collapse = ', '),"] is required by 'group.by0' but not present in the colnames of 'ldf'")
  if(any(!calculate['measure',] %in% c(as.vector(ldf[[measure.col]]), error_rate))) stop("'calculate' requires ",paste(calculate['measure',!calculate['measure',] %in% ldf[[measure.col]]], collapse = ',')," which does not exist in [",measure.col,"]")
  if(any(!unique(ldf[ldf[[measure.col]] %in% accuracy,value.col,drop=T]) %in% c(0,1,NA))) stop("The preference can only be coded as 0 (local preference) or 1 (global preference). Missing values must be NA.")
  if(!is.null(calculate.special)) if(!any(calculate.special %in% c('mean_IES','median_IES','RCS'))) stop("Special summary methods are only implemented for 'mean_IES', 'median_IES', and 'RCS'. To implement your own special summary methods, please edit this function")
  if(!is.numeric(ldf[[value.col]])) stop("The value column must be numeric")
  if((correct.trials.only)){
    if(!accuracy %in% ldf[[measure.col]]) stop("To remove incorrect trials (does not affect accuracy scores), accuracy measures for each trial need to be provieded.")
    if(any(!dplyr::filter(ldf, measure == accuracy)[[value.col]] %in% c(0,1,NA))) stop("'",accuracy,"' may only be 0, 1, or NA")}
  if(error_rate %in% colnames(calculate) & !error_rate %in% ldf[[measure.col]] & !accuracy %in% ldf[[measure.col]]) stop("'",error_rate,"' is defined in 'calculate' but '",error_rate,"' does not exist in the [",measure.col,"] and neither does '",accuracy,"', from which '",error_rate,"' could be calculated.")
  for(i in c('trial_type','sum.of.ACC','sum.of.RT')) if(i %in% colnames(ldf)) stop("'ldf' cannot contain a column called [",i,"], as it is used internally.")
  if(any(!make.trial_type %in% colnames(ldf))) stop("'make.trial_type' can only define column names that exist in ldf")
  if(length(make.trial_type) > 2) stop("'make.trial_type' can not define more than two columns")

  # Check trials are uniquely identified
  grpsz = ldf %>% dplyr::select(-!!dplyr::sym(value.col)) %>% dplyr::group_by_all() %>% dplyr::group_size()
  if(any(grpsz > 1)) stop(sum(grpsz - 1)," of ",sum(grpsz)," rows are duplicates of existing rows. Every row must be uniquely identified when using all columns except [",paste(level.col,congruence.col,value.col, sep = ','),"].")

  # Make the ldf wider and drop all rows that contain NA in the new accuracy-column
  wdf = ldf %>% tidyr::pivot_wider(names_from = all_of(measure.col), values_from = all_of(value.col))
  wdf[is.na(wdf[[accuracy]]),unique(ldf[[measure.col]])] = NA

  # If accuracy is present, calculate the error from the accuracy
  if(accuracy %in% colnames(wdf)) wdf[[error_rate]] = 1 - wdf[[accuracy]]

  # If required, trim the RT of wrong trials. Create wdf2 to use for the calculation of RCS
  wdf2 = wdf
  if(correct.trials.only) wdf[!wdf[[accuracy]] %in% 1, unique(ldf[[measure.col]])[!unique(ldf[[measure.col]]) %in% c(accuracy,error_rate)]] = NA

  # Create the power set of group.by (NULL if group.by is NULL)
  a = list(NULL)
  if(!is.null(group.by))
    for(j in 1:length(group.by)) a = c(a, combn(group.by, m=j, simplify=F))
  # Add group.by0 to each element of the power set
  a = lapply(a, function(vec) c(vec,group.by0))
  message('Provided grouping instructions'); print(a)

  A = data.frame(matrix(nrow = 1, ncol = length(group.by)+2))
  colnames(A) = c(group.by,measure.col,value.col)
  # For each summary method and each combination of grouping columns
  for (i in 1:ncol(calculate))
    for (j in 1:length(a))
      suppressMessages({
        # Group the value-col by all cols defined in group.by and then call the method specified in calculate on the measure specified in calculate
        A0 = wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(a[[j]]))) %>% dplyr::summarise(!!dplyr::sym(value.col) := do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T)))
        # Add a column with name of this measure and rowbind it to the main df
        A0[[measure.col]] = colnames(calculate)[i]
        A = dplyr::bind_rows(A, A0)})
  # Remove the first empty row
  A = A[-1,]

  # Create the trial_type and reorder the colums if required
  if(!is.null(make.trial_type)) {
    if(length(make.trial_type) == 1) A$trial_type = A[[make.trial_type]] else A$trial_type = ifelse(is.na(A$level) & is.na(A$congruence), 'all', ifelse(is.na(A$level), as.character(A$congruence),ifelse(is.na(A$congruence), as.character(A$level),paste(A$level, A$congruence, sep = "."))))
    A = A[,c(group.by0,group.by,'trial_type',measure.col,value.col)]} else A = A[,c(group.by0,group.by,measure.col,value.col)]

  return(as.data.frame(A))
}


#' A function to summarise trials of a local-global bias experiment.
#'
#' A function to calculate bias metrics from summary values in a standardised long dataframe. The preference-measure is treated differently from all other measures. Interpret the meaning of metrics carefully, especially what a certain combination of metric and measure mean. Generally, when a large input measure signifies bad performance (e.g. long RT), a resulting positive score indicates a global bias (e.g. in CPS) or a bias for congruent figures (e.g. in LIS), respectively.
#' @param ldf Long format dataframe with one column named 'value' containing the values, one identifier-column named 'measure' describing the summary measure of the value (from summarise.trials()), and one identifer-column named 'type' which merges the information from level and congruence (from summarise.trials()).
#' @param identifier.cols Columns that uniquely index all values with their identifiers. No default.
#' @param metrics A 2*x dataframe with instructions how to calculate metrics as difference or minimum between two summary values. Each column contains instructions for one metric. The column name is the name of the metric to be calculated, the first and second row (R1, R2) specify either a summary type from the type-column in 'ldf' or a metric that was previously calculated, the third row specifies the method to be used to calculate the metric from the entries in row 1 and 2. Currently, only 'diff'  (score = R1 - R2) and 'min' (score = min(R1, R2, na.rm = F)) are implemented. When the function does not find the inputs required by a metric instruction (R1, R2), the metric is ignored. Defaults to default.metrics(), which returns a dataframe with instructios for default metrics, using the default summary types (con, inc, loc, glo, con.loc, con.glo, etc.).
#' @param custom.metrics A 2*x dataframe with an equivalent layout as default metrics, containing custom metric names and custom instructions. If a column name of 'custom.metrics' exists in 'default.metrics', it replaces the column, otherwise it is appended. A sorting algorithmus ensures that instructions are appended such that when a metric is used as minuend or subtrahend, this metric exists by the time it is required. Defaults to NULL.
#' @param preference.metrics A 2*x dataframe with an equivalent layout as default metrics, containing instructions for the calculation of preference-metrics. When the second and third row are NA, the metric as defined by the column name is identical to the value of the type/metric defined by the first row. Defaults to preference.metrics().
#' @param preference The identifiers in the measure-column used to index preference-summaries. No default.
#' @param notify Should progress information be provided. Defaults to TRUE.
#' @keywords summary
#' @export
#' @examples
#' # Calculate trial type summaries from example.performance.data() with summarise.trials() and then calculate scores from these summaries
#' score.summaries(ldf = summarise.trials(wdf = example.performance.data()))
#' # Add a custom metric to the calculation instructions: Additionally to all default metrics, the metric NEW should be calculated as difference between 'BIS' and 'SIS' and 'OPS' (defined in default.metrics) should now be the min() of 'loc' and 'glo' instead of the difference
#' score.summaries(ldf = summarise.trials(wdf = example.performance.data()), custom.metrics = data.frame(NEW = c('BIS','SIS','diff'), OPS = c('loc','glo','min')))
#' # Calculate only OPS as difference between local and global trials
#' score.summaries(ldf = summarise.trials(wdf = example.performance.data()), metrics = data.frame(OPS = c('loc','glo','diff')))
#' summaries = summarise.trials.PREF(wdf = example.preference.data(),group.by0 = c('session','id','level','congruence'), make.trial_type = c('level','congruence'))
#' score.summaries(ldf = summaries, identifier.cols = c('session','id'), preference = c('mean_PREF','sum_PREF'))
#' debug(score.summaries)
score.summaries0 = function(ldf, identifier.cols, metrics = LoGloBias::default.metrics(), preference.metrics = LoGloBias::preference.metrics(), custom.metrics = NULL, preference, notify = T){

  if(any(!c('value','measure','trial_type') %in% colnames(ldf))) stop("'ldf' must contain cols named 'value', 'measure', and 'trial_type'")
  # if(!'trial_type' %in% identifier.cols) identifier.cols = c(identifier.cols,'trial_type')
  if(!'measure' %in% identifier.cols) identifier.cols = c(identifier.cols,'measure')
  # TODO: Check if all groupings are equally-sized
  # TODO: Check if score.summaries requires equally-sized groupings

  ldf.type = ldf
  ind.type = lapply(ldf.type[,c(identifier.cols,'trial_type')], unique)

  # If present, separate the preference values from other values
  if(any(preference %in% ldf.type$measure)) do.pref = T else do.pref = F
  if(do.pref){
    if(notify) message("'",paste(preference,collapse = ','),"' found in column [measure]. Using separate calculation ('preference.metrics()') for the preference-based scores.")
    ldf.type.P = ldf.type[ ldf.type$measure %in% preference,]
    ldf.type  = ldf.type[!ldf.type$measure %in% preference,]
    ind.type.P = lapply(ldf.type.P[,c(identifier.cols,'trial_type')], unique)
    ind.type  = lapply(ldf.type[ ,c(identifier.cols,'trial_type')], unique)}

  # If the trial_types or metrics required for the calculation of a metric is neither present in ldf.type$trial_types nor in colnames(metrics), remove the metric from the list
  for (i in colnames(metrics)) if(any(!metrics[1:2,i] %in% ldf.type$trial_type)) if(any(!metrics[1:2,i] %in% colnames(metrics))) metrics[[i]] = NULL
  if(ncol(metrics)==0) {message("Metrics as instructed:"); print(metrics); stop("None of the trial trial_type names required were found in ldf. Please \n - either adhere to the naming convention 'loc' (local), 'glo' (global), 'con' (congruent), 'inc' (incongruent), 'ctr' (control), as well as 'loc.con' (local congruent), etc. \n - or redefine the names of the trial_types in by using 'custom.metrics'")}
  if(do.pref) {
    for (i in colnames(preference.metrics)) if(any(!preference.metrics[1:2,i] %in% c(ldf.type.P$trial_type,NA))) if(any(!preference.metrics[1:2,i] %in% colnames(preference.metrics))) preference.metrics[[i]] = NULL
    if(ncol(preference.metrics)==0) {message("Preference metrics:"); print(preference.metrics); stop("None of the trial trial_type names required were found in ldf. Please \n - either adhere to the naming convention 'loc' (local), 'glo' (global), 'con' (congruent), 'inc' (incongruent), 'ctr' (control), as well as 'loc.con' (local congruent), etc. \n - or redefine the names of the trial_types in by using 'custom.metrics'")}}

  # Check if the custom metrics use existing trial_types/metrics, add the custom metrics to the metrics dataframe, and then check if there are any self-referential instructions in the new
  if(!is.null(custom.metrics)) if(!is.data.frame(custom.metrics)) {message("'custom.metrics' must be a dataframe with 3 rows, to amend or append the default metrics:"); print(metrics); stop()
  } else {
    # Check if any calculations require non-existent trial_types/metrics
    val.exist = as.matrix(custom.metrics[1:2,]) %in% ldf.type$trial_type | as.matrix(custom.metrics[1:2,]) %in% colnames(cbind(metrics,custom.metrics))
    if(any(!val.exist)) {print(as.matrix(custom.metrics)[1:2,val.exist]); stop("These trial_types/metrics required for the calculation of custom metrics do not exist in ldf$trial_types / will not be be calculated")}
    # If a column in custom.metrics redefines an existing metric, replace the column in metrics, otherwise append it
    for (i in colnames(custom.metrics)) if(i %in% colnames(metrics)) metrics[,i] = custom.metrics[,i] else metrics = cbind(metrics, custom.metrics[i])
    # Go through the cols from left to right with a position-counter until all cols were checked or until the position-counter has remained static for the last x iterations (x is the size of the remaining cols)
    previous = -ncol(metrics):0; position = 1
    while(position <= ncol(metrics) & !any(duplicated(previous[(length(previous)-(length(metrics)-position+1)):length(previous)]))){
      # If both values required for the calculation of the current col are present in ldf.type$trial_type or are metrics that were checked before, do nothing and increase the position-counter by 1, else, extract this column and append it to the end of the dataframe, to be checked again later
      if(all(metrics[1:2,position] %in% ldf.type$trial_type | metrics[1:2,position] %in% colnames(metrics)[1:position][-1])) position = position + 1 else metrics = cbind(metrics[,colnames(metrics) != i], metrics[i])
      # Then, append the position counter so that the while look can check if progress is being made
      previous = c(previous,position-1)}
    if(position <= ncol(metrics)) {print(metrics); stop("It seems that 'custom.metrics' requires self-referential calculations from metrics. Make sure that no metric A is calculated from metrics B,C, which themselves require metric A to be calculated.")}}

  # Notify of the calculations
  if(notify) {
    if(do.pref)                    {message("Preference calculation:");           print(preference.metrics)}
    if(any(metrics[3,] == 'diff')) {message("Metrics calculated as difference:"); print(metrics[,metrics[3,] == 'diff', drop = F], row.names = c('R1','R2','method'))}
    if(any(metrics[3,] == 'min'))  {message("Metrics calculated as min():");      print(metrics[,metrics[3,] == 'min',  drop = F], row.names = c('R1','R2','method'))}}

  ldf.metric = data.frame()
  # For each col in the standard metrics
  for (i in colnames(metrics)) {
    ab.val = list()
    # For first and second row of metrics[[i]], check if they are a trial trial_type or a metric, find and store the corresponding rows from ldf.type in a list
    for(j in 1:2) if(metrics[[i]][j] %in% unique(ldf.type$trial_type)) {ab.val[[j]] = ldf.type[unique(ldf.type$trial_type) %in% metrics[[i]][j],]} else if(metrics[[i]][j] %in% unique(ldf.metric$metric)) {ab.val[[j]] = ldf.metric[ldf.metric$metric %in% metrics[[i]][j],]}
    # Check the size of each part and left_join the two by group.by and measure
    if(nrow(ab.val[[1]]) != nrow(ab.val[[2]])) stop("Issue when calculating '",i,"': '",metrics[[i]][1],"' and '",metrics[[i]][2],"' have different rownumbers.")
    # ab.val = dplyr::left_join(ab.val[[1]],ab.val[[2]], by = identifier.cols)
    # if(metrics[[i]][3] == 'diff') ab.val$value = ab.val$value.x - ab.val$value.y else if(metrics[[i]][3] == 'ratio') ab.val$value = ab.val$value.x / ab.val$value.y else if(metrics[[i]][3] == 'ratio') ab.val$value = min(ab.val$value.x,ab.val$value.y) else stop("The method-row in the metric-instructions can only be 'diff', 'ratio', 'min', or NA")
    # for(j in 1:2) ab.val[[j]] = ab.val[[j]] %>% dplyr::arrange(!!dplyr::syms(identifier.cols))
    for (j in identifier.cols) if(any(ab.val[[1]][[j]] != ab.val[[2]][[j]])) stop('Not properly sorted')
    # Calculate scores
    if(metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",metrics[[i]][3],"' is required to calculate '",colnames(metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
    val0 = ab.val[[1]][,identifier.cols]
    # Add metric name
    val0$metric = i
    val0$value = val

    # Check if the order of the rows (except value, metric, trial_type) is the same in R1 and R2
    # if(sum(!ab.val[[1]][,!colnames(ab.val[[1]]) %in% c('value', 'metric', 'trial_type')] == ab.val[[2]][,!colnames(ab.val[[2]]) %in% c('value', 'metric', 'trial_type')])>0) stop("The order of rows does not match in the calculation of ",i,". This error should not happen and possibly requires comprehensive troubleshooting of 'score.summaries()'")
    # For the id.dataframe extract the identifier-columns from the minuend ldfs, remove the trial_type-col (if it exists) and replace/add the target metric
    # id.df = ab.val[[1]][,!colnames(ab.val[[1]]) %in% c('value','trial_type')]
    # id.df$metric = i
    # If the metric i is not specified in metrics[[i]][3], calculate the difference between [[1]] and [[2]], otherwise take the min
    # if(metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",metrics[[i]][3],"' is required to calculate '",colnames(metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
    # cbind the resulting var-vector and the id.dataframe and rbind the it with the ldf containing the previous metrics
    ldf.metric = rbind(ldf.metric, val0)}

  ldf.metric.P = data.frame()
  # For each column in the preference metrics
  if(do.pref) for(i in colnames(preference.metrics)){
    ab.val = list()
    # For R1 and R2, check if they are a trial trial_type or a metric, find and store the corresponding rows from ldf.type in a list
    for(j in 1:2) if(is.na(preference.metrics[[i]][j])) ab.val[[j]] = NA else if(preference.metrics[[i]][j] %in% ind.type.P$trial_type) {ab.val[[j]] = ldf.type.P[ldf.type.P$trial_type %in% preference.metrics[[i]][j],]} else if(preference.metrics[[i]][j] %in% unique(ldf.metric.P$metric)) {ab.val[[j]] = ldf.metric.P[ldf.metric.P$metric %in% metrics[[i]][j],]}
    # Check if the order of the rows (except value, metric, trial_type) is the same in R1 and R2
    # if(!is.na(ab.val[1]) & !is.na(ab.val[2])) if(sum(!ab.val[[1]][,!colnames(ab.val[[1]]) %in% c('value', 'metric', 'trial_type')] == ab.val[[2]][,!colnames(ab.val[[2]]) %in% c('value', 'metric', 'trial_type')])>0) stop("The order of rows does not match in the calculation of ",i,". This error should not happen and possibly requires comprehensive troubleshooting of 'score.summaries()'")
    # For the id.dataframe extract the identifier-columns from the minuend ldfs, remove the trial_type-col (if it exists) and replace/add the target metric
    for(j in 1:2) if(!is.na(ab.val[j])) id.df = ab.val[[j]][,!colnames(ab.val[[j]]) %in% c('value','trial_type')]
    id.df$metric = i
    # If the metric i is not specified in metrics[[i]][3], calculate the difference between [[1]] and [[2]], otherwise take the min
    if(is.na(preference.metrics[[i]][3])) val = ab.val[[1]]$value else if(preference.metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(preference.metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",preference.metrics[[i]][3],"' is required to calculate '",colnames(preference.metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
    # cbind the resulting var-vector and the id.dataframe and rbind the it with the ldf containing the previous metrics
    ldf.metric.P = rbind(ldf.metric.P, cbind(id.df, value = val))}

  # Join the two score dataframes
  ldf.metric = rbind(ldf.metric, ldf.metric.P)

  # Arrange the order of columns and then order the rows by column from left to right
  cn = colnames(ldf.metric)[!colnames(ldf.metric) %in% c('measure','metric','value')]
  ldf.metric = ldf.metric[,c(cn,'measure','metric','value')]
  for (i in rev(colnames(ldf.metric)[!colnames(ldf.metric) %in% 'value'])) ldf.metric = ldf.metric[order(ldf.metric[[i]]),]

  return(ldf.metric)
}





#' A function to summarise trials of a local-global bias experiment.
#'
#' A function to calculate bias metrics from summary values in a standardised long dataframe. The preference-measure is treated differently from all other measures. Interpret the meaning of metrics carefully, especially what a certain combination of metric and measure mean. Generally, when a large input measure signifies bad performance (e.g. long RT), a resulting positive score indicates a global bias (e.g. in CPS) or a bias for congruent figures (e.g. in LIS), respectively.
#' @param ldf Long format dataframe with one column named 'value' containing the values, one identifier-column named 'measure' describing the summary measure of the value (from summarise.trials()), and one identifer-column named 'type' which merges the information from level and congruence (from summarise.trials()).
#' @param measure.cols Names of the columns containing the measures. No default. Values must be numeric.
#' @param identifier.cols Columns that uniquely identify all measures. Any column that is not defined here is ignored. No default.
#' @param metrics A 2*x dataframe with instructions how to calculate metrics as difference or minimum between two summary values. Each column contains instructions for one metric. The column name is the name of the metric to be calculated, the first and second row (R1, R2) specify either a summary type from the type-column in 'ldf' or a metric that was previously calculated, the third row specifies the method to be used to calculate the metric from the entries in row 1 and 2. Currently, only 'diff'  (score = R1 - R2) and 'min' (score = min(R1, R2, na.rm = F)) are implemented. When the function does not find the inputs required by a metric instruction (R1, R2), the metric is ignored. Defaults to default.metrics(), which returns a dataframe with instructios for default metrics, using the default summary types (con, inc, loc, glo, con.loc, con.glo, etc.).
#' @param custom.metrics A 2*x dataframe with an equivalent layout as default metrics, containing custom metric names and custom instructions. If a column name of 'custom.metrics' exists in 'default.metrics', it replaces the column, otherwise it is appended. A sorting algorithmus ensures that instructions are appended such that when a metric is used as minuend or subtrahend, this metric exists by the time it is required. Defaults to NULL.
#' @param preference.metrics A 2*x dataframe with an equivalent layout as default metrics, containing instructions for the calculation of preference-metrics. When the second and third row are NA, the metric as defined by the column name is identical to the value of the type/metric defined by the first row. Defaults to preference.metrics().
#' @param preference The identifier in the measure-column used to index preference-values. Defaults to 'PREF'.
#' @param notify Should progress information be provided. Defaults to TRUE.
#' @keywords summary
#' @export
#' @examples
#' # Calculate trial type summaries from example.performance.data() with summarise.trials() and then calculate scores from these summaries
#' score.summaries(ldf = summarise.trials(wdf = example.performance.data()))
#' # Add a custom metric to the calculation instructions: Additionally to all default metrics, the metric NEW should be calculated as difference between 'BIS' and 'SIS' and 'OPS' (defined in default.metrics) should now be the min() of 'loc' and 'glo' instead of the difference
#' score.summaries(ldf = summarise.trials(wdf = example.performance.data()), custom.metrics = data.frame(NEW = c('BIS','SIS','diff'), OPS = c('loc','glo','min')))
#' # Calculate only OPS as difference between local and global trials
#' score.summaries(ldf = summarise.trials(wdf = example.performance.data()), metrics = data.frame(OPS = c('loc','glo','diff')))
#' summaries = summarise.trials.PREF(wdf = example.preference.data(),group.by0 = c('session','id','level','congruence'), make.trial_type = c('level','congruence'))
#' score.summaries(ldf = summaries, preference = c('mean_PREF','sum_PREF'))
#' debug(score.summaries)
score.summaries2 = function(wdf, identifier.cols, measure.cols, metrics = LoGloBias::default.metrics(), preference.metrics = LoGloBias::preference.metrics(), custom.metrics = NULL, preference, notify = T){

  if(any(!identifier.cols %in% colnames(wdf))) stop("'wdf' must contain all column names defined by 'identifier.cols'")
  if(any(!measure.cols %in% colnames(wdf))) stop("'wdf' must contain all column names defined by 'identifier.cols'")
  if(!'trial_type' %in% identifier.cols) stop("'identifier.cols' must contain 'trial_type'")
  # TODO: Check if all groupings are equally-sized
  # TODO: Check if score.summaries requires equally-sized groupings

  ldf.type = wdf
  ind.type = lapply(ldf.type[,identifier.cols], unique)

  # If present, separate the preference values from other values
  if(any(preference %in% ldf.type[,measure.cols])) do.pref = T else do.pref = F
  if(do.pref){
    if(notify) message("'",paste(preference,collapse = ','),"' found in 'wdf'. Using separate calculation ('preference.metrics()') for the preference-based scores.")
    ldf.type.P = ldf.type[,c(identifier.cols,measure.cols[!measure.cols %in% preference])]
    ldf.type  = ldf.type[,c(identifier.cols,measure.cols[ measure.cols %in% preference])]
    ind.type.P = lapply(ldf.type.P[,identifier.cols], unique)
    ind.type  = lapply(ldf.type[ ,identifier.cols], unique)}

  # If the trial_types or metrics required for the calculation of a metric is neither present in ldf.type$trial_types nor in colnames(metrics), remove the metric from the list
  for (i in colnames(metrics)) if(any(!metrics[1:2,i] %in% ldf.type$trial_type)) if(any(!metrics[1:2,i] %in% colnames(metrics))) metrics[[i]] = NULL
  if(ncol(metrics)==0) {message("Metrics as instructed:"); print(metrics); stop("None of the trial trial_type names required were found in ldf. Please \n - either adhere to the naming convention 'loc' (local), 'glo' (global), 'con' (congruent), 'inc' (incongruent), 'ctr' (control), as well as 'loc.con' (local congruent), etc. \n - or redefine the names of the trial_types in by using 'custom.metrics'")}
  if(do.pref) {
    for (i in colnames(preference.metrics)) if(any(!preference.metrics[1:2,i] %in% c(ldf.type.P$trial_type,NA))) if(any(!preference.metrics[1:2,i] %in% colnames(preference.metrics))) preference.metrics[[i]] = NULL
    if(ncol(preference.metrics)==0) {message("Preference metrics:"); print(preference.metrics); stop("None of the trial trial_type names required were found in ldf. Please \n - either adhere to the naming convention 'loc' (local), 'glo' (global), 'con' (congruent), 'inc' (incongruent), 'ctr' (control), as well as 'loc.con' (local congruent), etc. \n - or redefine the names of the trial_types in by using 'custom.metrics'")}}

  # Notify of the calculations
  if(notify) {
    if(do.pref)                    {message("Preference calculation:");           print(preference.metrics)}
    if(any(metrics[3,] == 'diff')) {message("Metrics calculated as difference:"); print(metrics[,metrics[3,] == 'diff', drop = F], row.names = c('R1','R2','method'))}
    if(any(metrics[3,] == 'min'))  {message("Metrics calculated as min():");      print(metrics[,metrics[3,] == 'min',  drop = F], row.names = c('R1','R2','method'))}}

  ldf.metric = data.frame()
  # For each col in the standard metrics
  for (i in colnames(metrics)) {
    ab.val = list()
    # For first and second row of metrics[[i]], check if they are a trial trial_type or a metric, find and store the corresponding rows from ldf.type in a list
    for(j in 1:2) if(metrics[[i]][j] %in% ind.type$trial_type) {ab.val[[j]] = ldf.type[ldf.type$trial_type %in% metrics[[i]][j],]} else if(metrics[[i]][j] %in% unique(ldf.metric$metric)) {ab.val[[j]] = ldf.metric[ldf.metric$metric %in% metrics[[i]][j],]}
    # Leftjoin the two by group.by and measure
    if(nrow(ab.val[[1]]) != nrow(ab.val[[2]])) stop("Issue when calculating '",i,"': '",metrics[[i]][1],"' and '",metrics[[i]][2],"' have different rownumbers.")
    ab.val = dplyr::left_join(ab.val[[1]],ab.val[[2]], by = c(group.by,'measure'))
    if(metrics[[i]][3] == 'diff') ab.val$value = ab.val$value.x - ab.val$value.y else if(metrics[[i]][3] == 'ratio') ab.val$value = ab.val$value.x / ab.val$value.y else if(metrics[[i]][3] == 'ratio') ab.val$value = min(ab.val$value.x,ab.val$value.y) else stop("The method-row in the metric-instructions can only be 'diff', 'ratio', 'min', or NA")
    ab.val$metric = i
    ab.val = ab.val[,c(group.by,'measure','metric','value')]

    # Check if the order of the rows (except value, metric, trial_type) is the same in R1 and R2
    # if(sum(!ab.val[[1]][,!colnames(ab.val[[1]]) %in% c('value', 'metric', 'trial_type')] == ab.val[[2]][,!colnames(ab.val[[2]]) %in% c('value', 'metric', 'trial_type')])>0) stop("The order of rows does not match in the calculation of ",i,". This error should not happen and possibly requires comprehensive troubleshooting of 'score.summaries()'")
    # For the id.dataframe extract the identifier-columns from the minuend ldfs, remove the trial_type-col (if it exists) and replace/add the target metric
    id.df = ab.val[[1]][,!colnames(ab.val[[1]]) %in% c('value','trial_type')]
    id.df$metric = i
    # If the metric i is not specified in metrics[[i]][3], calculate the difference between [[1]] and [[2]], otherwise take the min
    if(metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",metrics[[i]][3],"' is required to calculate '",colnames(metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
    # cbind the resulting var-vector and the id.dataframe and rbind the it with the ldf containing the previous metrics
    ldf.metric = rbind(ldf.metric, cbind(id.df, value = val))}

  ldf.metric.P = data.frame()
  # For each column in the preference metrics
  if(do.pref) for(i in colnames(preference.metrics)){
    ab.val = list()
    # For R1 and R2, check if they are a trial trial_type or a metric, find and store the corresponding rows from ldf.type in a list
    for(j in 1:2) if(is.na(preference.metrics[[i]][j])) ab.val[[j]] = NA else if(preference.metrics[[i]][j] %in% ind.type.P$trial_type) {ab.val[[j]] = ldf.type.P[ldf.type.P$trial_type %in% preference.metrics[[i]][j],]} else if(preference.metrics[[i]][j] %in% unique(ldf.metric.P$metric)) {ab.val[[j]] = ldf.metric.P[ldf.metric.P$metric %in% metrics[[i]][j],]}
    # Check if the order of the rows (except value, metric, trial_type) is the same in R1 and R2
    # if(!is.na(ab.val[1]) & !is.na(ab.val[2])) if(sum(!ab.val[[1]][,!colnames(ab.val[[1]]) %in% c('value', 'metric', 'trial_type')] == ab.val[[2]][,!colnames(ab.val[[2]]) %in% c('value', 'metric', 'trial_type')])>0) stop("The order of rows does not match in the calculation of ",i,". This error should not happen and possibly requires comprehensive troubleshooting of 'score.summaries()'")
    # For the id.dataframe extract the identifier-columns from the minuend ldfs, remove the trial_type-col (if it exists) and replace/add the target metric
    for(j in 1:2) if(!is.na(ab.val[j])) id.df = ab.val[[j]][,!colnames(ab.val[[j]]) %in% c('value','trial_type')]
    id.df$metric = i
    # If the metric i is not specified in metrics[[i]][3], calculate the difference between [[1]] and [[2]], otherwise take the min
    if(is.na(preference.metrics[[i]][3])) val = ab.val[[1]]$value else if(preference.metrics[[i]][3] == 'diff') val = ab.val[[1]]$value - ab.val[[2]]$value else if(preference.metrics[[i]][3] == 'min') val = apply(cbind(ab.val[[1]]$value, ab.val[[2]]$value), 1, min) else stop("Method '",preference.metrics[[i]][3],"' is required to calculate '",colnames(preference.metrics)[i],"', but is not implemented. Use 'diff' or 'min'")
    # cbind the resulting var-vector and the id.dataframe and rbind the it with the ldf containing the previous metrics
    ldf.metric.P = rbind(ldf.metric.P, cbind(id.df, value = val))}

  # Join the two score dataframes
  ldf.metric = rbind(ldf.metric, ldf.metric.P)

  # Arrange the order of columns and then order the rows by column from left to right
  cn = colnames(ldf.metric)[!colnames(ldf.metric) %in% c('measure','metric','value')]
  ldf.metric = ldf.metric[,c(cn,'measure','metric','value')]
  for (i in rev(colnames(ldf.metric)[!colnames(ldf.metric) %in% 'value'])) ldf.metric = ldf.metric[order(ldf.metric[[i]]),]

  return(ldf.metric)
}





#' Summarise trials of a local-global preference bias experiment
#'
#' Group trials of a local-global bias experiment in which a participant's preference instead of accuracy is measured. Group trials in a long format dataframe and apply defineable summary statistic functions to the measures. A trial's level is defined by the choice the participant made.
#' @param ldf Long format dataframe with the trial data. One column must contain the data (value.col), one column must specify which measure the value is (measure.col), one column must specify the trial number (trial.col), and two further columns must specify the level and congruence of the trial, respectively. Further columns are used to group the data. The presence of reaction time and preference measures is assumed, the latter being mandatory.
#' @param value.col Name of the column containing the data. Defaults to 'value'.
#' @param trial.col,congruence.col,measure.col Names of the columns that identify a data-point: the trial it comes from, the congruence of that trial and which measure it is.
#' @param preference The name of the preference-identifier in the measure.col. Defaults to 'PREF'.
#' @param group.by Identifier-columns that are used create trial-groupings, within which statistics are calculated. Defaults to NULL, which means all columns except the value.col, measure.col, congruence.col, and trial.col are used.
#' @param calculate A 2*x dataframe with instructions how to summarise trials. Column names specify the name of the summary statistic, the first row specifies the measure to be summarised, and the second row the function to be used. The function must accept a vector and output a scalar. The first column must contain instructions to calculate the mean preference. Defaults to data.frame(PREF = c('PREF','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')).
#' @keywords summary
#' @export
#' @examples
#' # Summarise the example trial data by sessiona and id. Use all default settings.
#' ldf = summarise.preference.trials(ldf = example.preference.data(), group.by = c('session','id'))
#' # Since session and id are the only other columns besides the integral columns containing trial identifier, level and congruence identifier, the measure identifier, and the value, the result is the same without specifying the grouping
#' ldf = summarise.preference.trials(ldf = example.preference.data())
#' # Summarise the example trial data, group by id (i.e. pool sessions)
#' ldf = summarise.preference.trials(ldf = example.preference.data(), group.by = 'id')
#' # Note that if we do the same, but drop the session-column from the ldf, an error is thrown, because not all values are now uniquely identified, as each trial exists twice for each participant without a session-identifier to differentiate them
#' ldf = summarise.preference.trials(ldf = example.preference.data()[,-1], group.by = 'id')
#' # Calculate only sd of RT.
#' ldf = summarise.preference.trials(ldf = example.preference.data(), calculate = data.frame(SD = c('RT','sd')))
#' RCS is still automatically calculated. Disable by setting calculate.special to NULL.
#' ldf = summarise.preference.trials(ldf = example.preference.data(), calculate = data.frame(SD = c('RT','sd')), calculate.special = NULL)
summarise.preference.trials_long.df = function(ldf, group.by = NULL, calculate = data.frame(mean_PREF = c('PREF','mean'), mean_RT = c('RT','mean'), median_RT = c('RT','median')), trial.col = 'trial', measure.col = 'measure', congruence.col = 'congruence', value.col = 'value', preference = 'PREF', reaction_time = 'RT'){

  rownames(calculate) = c('measure','method')
  if(is.null(preference)) stop("'preference' cannot be NULL, as the preference-measure is integral to the processing of the data.")
  core.cols = list(trial.col = trial.col, measure.col = measure.col, congruence.col = congruence.col, value.col = value.col)
  for(i in names(core.cols)) if(!core.cols[[i]] %in% colnames(ldf)) stop("'",i,"' requires [",core.cols[[i]],"] but it does not exist in the column names of 'ldf'")
  if(!is.null(group.by)) if(any(!group.by %in% colnames(ldf))) stop("[",paste(group.by[!group.by %in% colnames(ldf)], collapse = ', '),"] is required by 'group.by' but not present in the colnames of 'ldf'")
  if(!preference %in% ldf[[measure.col]]) stop(preference," is required as preference measure but does not exist in the measure column [",measure.col,"] denoted by 'measure.col'")
  if(any(!calculate['measure',] %in% c(ldf$measure))) stop("'calculate' requires ",paste(calculate['measure',!calculate['measure',] %in% ldf$measure], collapse = ','),' which does not exist in the provided ldf$measure')
  if(any(calculate[[1]] != c(preference,'mean'))) stop("The first column of calculate must be used to describe the calcuation of the mean preference, as per default")
  if('new.level.column' %in% colnames(ldf)) stop("'ldf' cannot contain a column called [new.level.column], as this name is used for internal structuring.")
  if('type' %in% colnames(ldf)) stop("'ldf' cannot contain a column called [type], it will be the column name the trial type in the summarised ldf")
  if(any(!ldf[ldf[[measure.col]] %in% preference,][[value.col]] %in% c(1,-1,NA))) stop("Preference-values in the rows coded by '",preference,"' in [",measure.col,"] must be coded as 1 (global choice) or -1 (local choice). Missing values must be NA")

  # If no grouping was defined, use all cols except 'value','measure','level','congruence' and 'trial' for grouping
  if(is.null(group.by)) group.by = colnames(ldf)[!colnames(ldf) %in% c(value.col,measure.col,congruence.col,trial.col)]

  # Check trials are uniquely identified
  grpsz = ldf %>% dplyr::select(-!!dplyr::sym(value.col),-!!dplyr::sym(congruence.col)) %>% dplyr::group_by_all() %>% dplyr::group_size()
  if(any(grpsz > 1)) stop(sum(grpsz - 1)," of ",sum(grpsz)," rows are duplicates of existing rows. Every row must be uniquely identified when using all columns except [",paste(level.col,congruence.col,value.col, sep = ','),"].")

  # Make the ldf wider and drop all rows that contain NA in the new preference-column
  wdf = ldf %>% tidyr::pivot_wider(names_from = all_of(measure.col), values_from = all_of(value.col))
  wdf[is.na(wdf[[preference]]),unique(ldf[[measure.col]])] = NA

  # Calculate the mean preference for all trials and separately by congruence
  A1 = suppressMessages({wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(  group.by)))           %>% dplyr::summarise(value = do.call(match.fun(calculate['method',1]), list(!!dplyr::sym(calculate['measure',1]), na.rm = T)))})
  A2 = suppressMessages({wdf %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by, congruence.col)))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',1]), list(!!dplyr::sym(calculate['measure',1]), na.rm = T)))})
  # Make sure a column 'type' exists, containing the information of the type of trial used
  A1$type = 'all'
  colnames(A2)[colnames(A2) == congruence.col] = 'type'
  # Bind together and add a column for the measure
  A = rbind(A1, A2)
  A[[measure.col]] = preference

  # Recreate the standard level-column from the preference column
  wdf$new.level.column = ifelse(wdf[[preference]] == 1, 'glo', 'loc')
  # Create an alternate version without trials with undeterimned preference
  wdf2 = wdf %>% filter(!is.na(new.level.column))

  # For all other columns in the summary methods
  for (i in 2:ncol(calculate)) {suppressMessages({
    # Group the 'value' col by all cols defined in group.by / +level / +congruence / +level&congruence  and then call the method specified in calculate on the measure specified in calculate, and create a new column 'type' that merges information from level and congruence
    A1 = wdf  %>% dplyr::group_by(dplyr::across(dplyr::all_of(  group.by                                     ))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); A1$type = 'all'
    A2 = wdf2 %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by, 'new.level.column'                )))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); colnames(A2)[colnames(A2) == 'new.level.column'] = 'type'
    A3 = wdf  %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by,                     congruence.col)))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); colnames(A3)[colnames(A3) == congruence.col] = 'type'
    A4 = wdf2 %>% dplyr::group_by(dplyr::across(dplyr::all_of(c(group.by, 'new.level.column', congruence.col)))) %>% dplyr::summarise(value = do.call(match.fun(calculate['method',i]), list(!!dplyr::sym(calculate['measure',i]), na.rm = T))); A4$type = interaction(A4$new.level.column,A4[[congruence.col]]); A4[[congruence.col]] = NULL; A4$new.level.column = NULL
    Ax = rbind(A1, A2, A3, A4)
    Ax[[measure.col]] = colnames(calculate)[i]
    A = rbind(A, Ax)})}

  # Arrange the order of columns and then order the rows by column from left to right
  A = A[,c(group.by,'type',measure.col,value.col)]
  for (i in rev(colnames(A)[!colnames(A) %in% value.col])) A = A[order(A[[i]]),]

  return(as.data.frame(A))
}




