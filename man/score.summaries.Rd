% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LoGloBias.R
\name{score.summaries}
\alias{score.summaries}
\title{A function to summarise trials of a local-global bias experiment.}
\usage{
score.summaries(
  ldf,
  identifier.cols,
  metrics = LoGloBias::default.metrics(),
  preference.metrics = LoGloBias::preference.metrics(),
  preference = NULL,
  measure.col = "measure",
  trial_type.col = "trial_type",
  notify = T
)
}
\arguments{
\item{ldf}{Long format dataframe with one column named 'value' containing the values, one identifier-column named 'measure' describing the summary measure of the value (from summarise.trials()), and one identifer-column named 'type' which merges the information from level and congruence (from summarise.trials()).}

\item{identifier.cols}{Columns that uniquely index all values with their identifiers. No default.}

\item{metrics}{A 2*x dataframe with instructions how to calculate metrics as difference or minimum between two summary values. Each column contains instructions for one metric. The column name is the name of the metric to be calculated, the first and second row (R1, R2) specify either a summary type from the type-column in 'ldf' or a metric that was previously calculated, the third row specifies the method to be used to calculate the metric from the entries in row 1 and 2. Currently, only 'diff'  (score = R1 - R2) and 'min' (score = min(R1, R2, na.rm = F)) are implemented. When the function does not find the inputs required by a metric instruction (R1, R2), the metric is ignored. Defaults to default.metrics(), which returns a dataframe with instructios for default metrics, using the default summary types (con, inc, loc, glo, con.loc, con.glo, etc.).}

\item{preference.metrics}{A 2*x dataframe with an equivalent layout as default metrics, containing instructions for the calculation of preference-metrics. When the second and third row are NA, the metric as defined by the column name is identical to the value of the type/metric defined by the first row. Defaults to preference.metrics().}

\item{preference}{The identifiers in the measure-column used to index preference-summaries. No default.}

\item{notify}{Should progress information be provided. Defaults to TRUE.}

\item{custom.metrics}{A 2*x dataframe with an equivalent layout as default metrics, containing custom metric names and custom instructions. If a column name of 'custom.metrics' exists in 'default.metrics', it replaces the column, otherwise it is appended. A sorting algorithmus ensures that instructions are appended such that when a metric is used as minuend or subtrahend, this metric exists by the time it is required. Defaults to NULL.}
}
\description{
A function to calculate bias metrics from summary values in a standardised long dataframe. The preference-measure is treated differently from all other measures. Interpret the meaning of metrics carefully, especially what a certain combination of metric and measure mean. Generally, when a large input measure signifies bad performance (e.g. long RT), a resulting positive score indicates a global bias (e.g. in CPS) or a bias for congruent figures (e.g. in LIS), respectively.
}
\examples{
summaries.perf = summarise.trials(     wdf = example.performance.data(), group.by = c('session','id','level','congruence'), make.trial_type = c('level','congruence'))
summaries.pref = summarise.trials.PREF(wdf = example.preference.data(),group.by0 = c('session','id','level','congruence'), make.trial_type = c('level','congruence'))
# Calculate trial type summaries from example.performance.data() with summarise.trials() and then calculate scores from these summaries
ldf = score.summaries(ldf = summaries.perf, identifier.cols = c('session','id','measure'))
# Calculate only OPS as difference between local and global trials
ldf = score.summaries(ldf = summaries.perf, identifier.cols = c('session','id','measure'), metrics = data.frame(OPS = c('loc','glo','diff')))
ldf = score.summaries(ldf = summaries, identifier.cols = c('session','id'), preference = c('mean_PREF','sum_PREF'))
debug(score.summaries)
}
\keyword{summary}
